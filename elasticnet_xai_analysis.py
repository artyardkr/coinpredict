#!/usr/bin/env python3
"""
ElasticNet XAI (ì„¤ëª…ê°€ëŠ¥í•œ AI) ë¶„ì„

1. ê³„ìˆ˜ (Coefficients) ë¶„ì„
2. Feature Importance
3. SHAP Values
4. ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…
5. ë³€ìˆ˜ ê°„ ìƒí˜¸ì‘ìš©
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import shap
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

print("="*80)
print("ElasticNet XAI ë¶„ì„")
print("="*80)

# ========================================
# 1. ë°ì´í„° ì¤€ë¹„ ë° ëª¨ë¸ í•™ìŠµ
# ========================================
print("\n[1/6] ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ í•™ìŠµ...")

df = pd.read_csv('integrated_data_full.csv')
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values('Date').reset_index(drop=True)

# Target
df['target'] = df['Close'].shift(-1)
df = df[:-1].copy()

# Feature ì„ íƒ (step25ì™€ ë™ì¼)
exclude_cols = [
    'Date', 'Close', 'High', 'Low', 'Open', 'target',
    'cumulative_return', 'bc_market_price', 'bc_market_cap',
]

ema_sma_cols = [col for col in df.columns if ('EMA' in col or 'SMA' in col) and 'close' in col.lower()]
exclude_cols.extend(ema_sma_cols)
bb_cols = [col for col in df.columns if col.startswith('BB_')]
exclude_cols.extend(bb_cols)
exclude_cols = list(set(exclude_cols))

feature_cols = [col for col in df.columns if col not in exclude_cols]

for col in feature_cols:
    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

print(f"Features: {len(feature_cols)}ê°œ")

# Train/Test Split (70/30)
split_idx = int(len(df) * 0.7)
split_date = df['Date'].iloc[split_idx]

X_train = df.iloc[:split_idx][feature_cols].values
X_test = df.iloc[split_idx:][feature_cols].values
y_train = df.iloc[:split_idx]['target'].values
y_test = df.iloc[split_idx:]['target'].values

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ElasticNet í•™ìŠµ
print("ElasticNet í•™ìŠµ ì¤‘...")
model = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000, random_state=42)
model.fit(X_train_scaled, y_train)

# ì˜ˆì¸¡
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

# ì„±ëŠ¥
r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

print(f"\nì„±ëŠ¥:")
print(f"  Train RÂ²: {r2_train:.4f}")
print(f"  Test RÂ²:  {r2_test:.4f}")
print(f"  Test RMSE: ${rmse_test:,.2f}")

# ========================================
# 2. ê³„ìˆ˜ (Coefficients) ë¶„ì„
# ========================================
print("\n" + "="*60)
print("[2/6] ê³„ìˆ˜ ë¶„ì„")
print("="*60)

# ê³„ìˆ˜ ì¶”ì¶œ
coefficients = model.coef_
intercept = model.intercept_

print(f"\nIntercept (ì ˆí¸): ${intercept:,.2f}")
print(f"Non-zero ê³„ìˆ˜: {np.sum(coefficients != 0)}/{len(coefficients)}")

# ê³„ìˆ˜ DataFrame
coef_df = pd.DataFrame({
    'Feature': feature_cols,
    'Coefficient': coefficients,
    'Abs_Coefficient': np.abs(coefficients)
}).sort_values('Abs_Coefficient', ascending=False)

# 0ì´ ì•„ë‹Œ ê³„ìˆ˜ë§Œ
coef_df_nonzero = coef_df[coef_df['Coefficient'] != 0].copy()

print(f"\nTop 10 ì¤‘ìš” ë³€ìˆ˜ (ì ˆëŒ“ê°’ ê¸°ì¤€):")
for i, row in coef_df_nonzero.head(10).iterrows():
    sign = 'ğŸ“ˆ' if row['Coefficient'] > 0 else 'ğŸ“‰'
    print(f"  {sign} {row['Feature']:<30s}: {row['Coefficient']:+.4f}")

print(f"\nì–‘ìˆ˜ ê³„ìˆ˜ (ê°€ê²© ìƒìŠ¹ ìš”ì¸): {np.sum(coefficients > 0)}ê°œ")
print(f"ìŒìˆ˜ ê³„ìˆ˜ (ê°€ê²© í•˜ë½ ìš”ì¸): {np.sum(coefficients < 0)}ê°œ")
print(f"0 ê³„ìˆ˜ (ì œê±°ëœ ë³€ìˆ˜): {np.sum(coefficients == 0)}ê°œ")

# ========================================
# 3. Feature Importance
# ========================================
print("\n" + "="*60)
print("[3/6] Feature Importance")
print("="*60)

# Standardized ê³„ìˆ˜ (ì¤‘ìš”ë„)
# í‘œì¤€í™”ëœ ë°ì´í„°ì—ì„œ í•™ìŠµí–ˆìœ¼ë¯€ë¡œ, ê³„ìˆ˜ì˜ ì ˆëŒ“ê°’ì´ ê³§ ì¤‘ìš”ë„
feature_importance = np.abs(coefficients)
importance_df = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

# ëˆ„ì  ì¤‘ìš”ë„
importance_df['Cumulative'] = importance_df['Importance'].cumsum() / importance_df['Importance'].sum()

# 80% ì„¤ëª…ë ¥ì„ ìœ„í•œ ë³€ìˆ˜ ê°œìˆ˜
n_features_80 = (importance_df['Cumulative'] <= 0.8).sum() + 1

print(f"\nìƒìœ„ 10ê°œ ë³€ìˆ˜ ì¤‘ìš”ë„:")
for i, row in importance_df.head(10).iterrows():
    print(f"  {row['Feature']:<30s}: {row['Importance']:.4f} (ëˆ„ì : {row['Cumulative']*100:.1f}%)")

print(f"\n80% ì„¤ëª…ë ¥ì„ ìœ„í•œ ë³€ìˆ˜ ê°œìˆ˜: {n_features_80}ê°œ (ì´ {len(feature_cols)}ê°œ ì¤‘)")

# ========================================
# 4. SHAP Values ë¶„ì„
# ========================================
print("\n" + "="*60)
print("[4/6] SHAP Values ë¶„ì„")
print("="*60)

print("SHAP Explainer ìƒì„± ì¤‘...")
# Linear explainer (ë¹ ë¦„)
explainer = shap.LinearExplainer(model, X_train_scaled)

# Test set SHAP values
print("SHAP values ê³„ì‚° ì¤‘ (Test set)...")
shap_values_test = explainer.shap_values(X_test_scaled)

# SHAP í‰ê·  ì ˆëŒ“ê°’ (ì „ì—­ ì¤‘ìš”ë„)
shap_importance = np.abs(shap_values_test).mean(axis=0)
shap_df = pd.DataFrame({
    'Feature': feature_cols,
    'SHAP_Importance': shap_importance
}).sort_values('SHAP_Importance', ascending=False)

print(f"\nTop 10 SHAP ì¤‘ìš”ë„:")
for i, row in shap_df.head(10).iterrows():
    print(f"  {row['Feature']:<30s}: {row['SHAP_Importance']:.4f}")

# ========================================
# 5. ê°œë³„ ì˜ˆì¸¡ ì„¤ëª… (ìƒ˜í”Œ 3ê°œ)
# ========================================
print("\n" + "="*60)
print("[5/6] ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…")
print("="*60)

# 3ê°€ì§€ ì¼€ì´ìŠ¤: ì •í™•, ê³¼ëŒ€í‰ê°€, ê³¼ì†Œí‰ê°€
errors = y_pred_test - y_test
abs_errors = np.abs(errors)

# ê°€ì¥ ì •í™•í•œ ì˜ˆì¸¡
best_idx = np.argmin(abs_errors)
# ê°€ì¥ ê³¼ëŒ€í‰ê°€ (ì˜ˆì¸¡ >> ì‹¤ì œ)
overpredict_idx = np.argmax(errors)
# ê°€ì¥ ê³¼ì†Œí‰ê°€ (ì˜ˆì¸¡ << ì‹¤ì œ)
underpredict_idx = np.argmin(errors)

sample_indices = [best_idx, overpredict_idx, underpredict_idx]
sample_names = ['ê°€ì¥ ì •í™•', 'ê³¼ëŒ€í‰ê°€', 'ê³¼ì†Œí‰ê°€']

for idx, name in zip(sample_indices, sample_names):
    print(f"\n[{name}]")
    print(f"  ì‹¤ì œ: ${y_test[idx]:,.2f}")
    print(f"  ì˜ˆì¸¡: ${y_pred_test[idx]:,.2f}")
    print(f"  ì˜¤ì°¨: ${errors[idx]:+,.2f}")

    # Top 5 ê¸°ì—¬ ë³€ìˆ˜
    sample_shap = shap_values_test[idx]
    sample_shap_df = pd.DataFrame({
        'Feature': feature_cols,
        'SHAP': sample_shap
    }).sort_values('SHAP', key=lambda x: abs(x), ascending=False)

    print(f"  Top 5 ê¸°ì—¬ ë³€ìˆ˜:")
    for i, row in sample_shap_df.head(5).iterrows():
        direction = 'â¬†ï¸' if row['SHAP'] > 0 else 'â¬‡ï¸'
        print(f"    {direction} {row['Feature']:<25s}: {row['SHAP']:+.2f}")

# ========================================
# 6. ì‹œê°í™”
# ========================================
print("\n" + "="*60)
print("[6/6] ì‹œê°í™” ìƒì„± ì¤‘...")
print("="*60)

fig = plt.figure(figsize=(22, 16))

# (1) Top 20 ê³„ìˆ˜
ax1 = plt.subplot(4, 4, 1)
top_coef = coef_df_nonzero.head(20).sort_values('Coefficient')
colors = ['red' if x < 0 else 'green' for x in top_coef['Coefficient']]
ax1.barh(range(len(top_coef)), top_coef['Coefficient'], color=colors, alpha=0.7)
ax1.set_yticks(range(len(top_coef)))
ax1.set_yticklabels(top_coef['Feature'], fontsize=8)
ax1.set_xlabel('ê³„ìˆ˜ (Coefficient)', fontweight='bold')
ax1.set_title('Top 20 ë³€ìˆ˜ ê³„ìˆ˜', fontsize=12, fontweight='bold')
ax1.axvline(0, color='black', linewidth=1)
ax1.grid(True, alpha=0.3, axis='x')

# (2) Feature Importance (ì ˆëŒ“ê°’)
ax2 = plt.subplot(4, 4, 2)
top_importance = importance_df.head(20)
ax2.barh(range(len(top_importance)), top_importance['Importance'], color='steelblue', alpha=0.7)
ax2.set_yticks(range(len(top_importance)))
ax2.set_yticklabels(top_importance['Feature'], fontsize=8)
ax2.set_xlabel('ì¤‘ìš”ë„ (|ê³„ìˆ˜|)', fontweight='bold')
ax2.set_title('Top 20 Feature Importance', fontsize=12, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='x')

# (3) ëˆ„ì  ì¤‘ìš”ë„
ax3 = plt.subplot(4, 4, 3)
ax3.plot(range(1, len(importance_df)+1), importance_df['Cumulative'], linewidth=2, color='purple')
ax3.axhline(0.8, color='red', linestyle='--', linewidth=1, label='80%')
ax3.axvline(n_features_80, color='red', linestyle='--', linewidth=1)
ax3.set_xlabel('ë³€ìˆ˜ ê°œìˆ˜', fontweight='bold')
ax3.set_ylabel('ëˆ„ì  ì¤‘ìš”ë„', fontweight='bold')
ax3.set_title('ëˆ„ì  Feature Importance', fontsize=12, fontweight='bold')
ax3.legend()
ax3.grid(True, alpha=0.3)
ax3.text(n_features_80, 0.85, f'{n_features_80}ê°œ', ha='center', fontweight='bold')

# (4) ê³„ìˆ˜ ë¶„í¬
ax4 = plt.subplot(4, 4, 4)
ax4.hist(coefficients[coefficients != 0], bins=30, color='steelblue', alpha=0.7, edgecolor='black')
ax4.axvline(0, color='red', linestyle='--', linewidth=2)
ax4.set_xlabel('ê³„ìˆ˜ ê°’', fontweight='bold')
ax4.set_ylabel('ë¹ˆë„', fontweight='bold')
ax4.set_title('ê³„ìˆ˜ ë¶„í¬ (Non-zero)', fontsize=12, fontweight='bold')
ax4.grid(True, alpha=0.3, axis='y')

# (5) SHAP Summary Plot
ax5 = plt.subplot(4, 4, (5, 6))
shap.summary_plot(shap_values_test, X_test_scaled, feature_names=feature_cols,
                  show=False, max_display=15, plot_size=(8, 5))
plt.title('SHAP Summary Plot (Top 15)', fontsize=12, fontweight='bold')

# (6) SHAP Bar Plot
ax6 = plt.subplot(4, 4, (7, 8))
top_shap = shap_df.head(20)
ax6.barh(range(len(top_shap)), top_shap['SHAP_Importance'], color='coral', alpha=0.7)
ax6.set_yticks(range(len(top_shap)))
ax6.set_yticklabels(top_shap['Feature'], fontsize=8)
ax6.set_xlabel('í‰ê·  |SHAP Value|', fontweight='bold')
ax6.set_title('Top 20 SHAP Importance', fontsize=12, fontweight='bold')
ax6.grid(True, alpha=0.3, axis='x')

# (7) ê°œë³„ ì˜ˆì¸¡ 1: ì •í™•
ax7 = plt.subplot(4, 4, 9)
idx = sample_indices[0]
sample_shap = shap_values_test[idx]
sample_shap_df = pd.DataFrame({
    'Feature': feature_cols,
    'SHAP': sample_shap
}).sort_values('SHAP', key=lambda x: abs(x), ascending=False).head(10)
colors_sample = ['red' if x < 0 else 'green' for x in sample_shap_df['SHAP']]
ax7.barh(range(len(sample_shap_df)), sample_shap_df['SHAP'], color=colors_sample, alpha=0.7)
ax7.set_yticks(range(len(sample_shap_df)))
ax7.set_yticklabels(sample_shap_df['Feature'], fontsize=7)
ax7.set_xlabel('SHAP Value', fontweight='bold')
ax7.set_title(f'ê°€ì¥ ì •í™•í•œ ì˜ˆì¸¡\nì‹¤ì œ: ${y_test[idx]:,.0f}, ì˜ˆì¸¡: ${y_pred_test[idx]:,.0f}',
             fontsize=10, fontweight='bold')
ax7.axvline(0, color='black', linewidth=1)
ax7.grid(True, alpha=0.3, axis='x')

# (8) ê°œë³„ ì˜ˆì¸¡ 2: ê³¼ëŒ€í‰ê°€
ax8 = plt.subplot(4, 4, 10)
idx = sample_indices[1]
sample_shap = shap_values_test[idx]
sample_shap_df = pd.DataFrame({
    'Feature': feature_cols,
    'SHAP': sample_shap
}).sort_values('SHAP', key=lambda x: abs(x), ascending=False).head(10)
colors_sample = ['red' if x < 0 else 'green' for x in sample_shap_df['SHAP']]
ax8.barh(range(len(sample_shap_df)), sample_shap_df['SHAP'], color=colors_sample, alpha=0.7)
ax8.set_yticks(range(len(sample_shap_df)))
ax8.set_yticklabels(sample_shap_df['Feature'], fontsize=7)
ax8.set_xlabel('SHAP Value', fontweight='bold')
ax8.set_title(f'ê³¼ëŒ€í‰ê°€ (ì˜ˆì¸¡ > ì‹¤ì œ)\nì‹¤ì œ: ${y_test[idx]:,.0f}, ì˜ˆì¸¡: ${y_pred_test[idx]:,.0f}',
             fontsize=10, fontweight='bold')
ax8.axvline(0, color='black', linewidth=1)
ax8.grid(True, alpha=0.3, axis='x')

# (9) ê°œë³„ ì˜ˆì¸¡ 3: ê³¼ì†Œí‰ê°€
ax9 = plt.subplot(4, 4, 11)
idx = sample_indices[2]
sample_shap = shap_values_test[idx]
sample_shap_df = pd.DataFrame({
    'Feature': feature_cols,
    'SHAP': sample_shap
}).sort_values('SHAP', key=lambda x: abs(x), ascending=False).head(10)
colors_sample = ['red' if x < 0 else 'green' for x in sample_shap_df['SHAP']]
ax9.barh(range(len(sample_shap_df)), sample_shap_df['SHAP'], color=colors_sample, alpha=0.7)
ax9.set_yticks(range(len(sample_shap_df)))
ax9.set_yticklabels(sample_shap_df['Feature'], fontsize=7)
ax9.set_xlabel('SHAP Value', fontweight='bold')
ax9.set_title(f'ê³¼ì†Œí‰ê°€ (ì˜ˆì¸¡ < ì‹¤ì œ)\nì‹¤ì œ: ${y_test[idx]:,.0f}, ì˜ˆì¸¡: ${y_pred_test[idx]:,.0f}',
             fontsize=10, fontweight='bold')
ax9.axvline(0, color='black', linewidth=1)
ax9.grid(True, alpha=0.3, axis='x')

# (10) ê³„ìˆ˜ vs SHAP ë¹„êµ
ax10 = plt.subplot(4, 4, 12)
comparison_df = coef_df.merge(shap_df, on='Feature')
comparison_df = comparison_df[comparison_df['Abs_Coefficient'] > 0]
ax10.scatter(comparison_df['Abs_Coefficient'], comparison_df['SHAP_Importance'],
            alpha=0.5, s=50, color='purple')
ax10.set_xlabel('ê³„ìˆ˜ ì ˆëŒ“ê°’', fontweight='bold')
ax10.set_ylabel('SHAP ì¤‘ìš”ë„', fontweight='bold')
ax10.set_title('ê³„ìˆ˜ vs SHAP ì¤‘ìš”ë„', fontsize=12, fontweight='bold')
ax10.grid(True, alpha=0.3)

# ìƒê´€ê³„ìˆ˜
corr = comparison_df['Abs_Coefficient'].corr(comparison_df['SHAP_Importance'])
ax10.text(0.05, 0.95, f'ìƒê´€ê³„ìˆ˜: {corr:.3f}', transform=ax10.transAxes,
         fontsize=10, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# (11) ì–‘ìˆ˜/ìŒìˆ˜ ê³„ìˆ˜ ë¹„ìœ¨
ax11 = plt.subplot(4, 4, 13)
coef_stats = [
    np.sum(coefficients > 0),
    np.sum(coefficients < 0),
    np.sum(coefficients == 0)
]
labels_coef = ['ì–‘ìˆ˜\n(ìƒìŠ¹ìš”ì¸)', 'ìŒìˆ˜\n(í•˜ë½ìš”ì¸)', '0\n(ì œê±°)']
colors_coef = ['green', 'red', 'gray']
bars = ax11.bar(labels_coef, coef_stats, color=colors_coef, alpha=0.7)
for bar, val in zip(bars, coef_stats):
    ax11.text(bar.get_x() + bar.get_width()/2, val, f'{val}ê°œ',
            ha='center', va='bottom', fontweight='bold')
ax11.set_ylabel('ë³€ìˆ˜ ê°œìˆ˜', fontweight='bold')
ax11.set_title('ê³„ìˆ˜ ë¶€í˜¸ ë¶„í¬', fontsize=12, fontweight='bold')
ax11.grid(True, alpha=0.3, axis='y')

# (12) ìš”ì•½
ax12 = plt.subplot(4, 4, (14, 16))
ax12.axis('off')
summary = f"""
ã€ElasticNet XAI ë¶„ì„ ìš”ì•½ã€‘

1. ëª¨ë¸ ì„±ëŠ¥:
   Train RÂ²: {r2_train:.4f}
   Test RÂ²:  {r2_test:.4f}
   Test RMSE: ${rmse_test:,.2f}

2. ë³€ìˆ˜ ì„ íƒ:
   ì´ ë³€ìˆ˜: {len(feature_cols)}ê°œ
   ì‚¬ìš© ë³€ìˆ˜: {np.sum(coefficients != 0)}ê°œ
   ì œê±° ë³€ìˆ˜: {np.sum(coefficients == 0)}ê°œ
   ì œê±°ìœ¨: {np.sum(coefficients == 0)/len(coefficients)*100:.1f}%

3. Top 5 ì¤‘ìš” ë³€ìˆ˜:
   {coef_df_nonzero.iloc[0]['Feature'][:25]}: {coef_df_nonzero.iloc[0]['Coefficient']:+.4f}
   {coef_df_nonzero.iloc[1]['Feature'][:25]}: {coef_df_nonzero.iloc[1]['Coefficient']:+.4f}
   {coef_df_nonzero.iloc[2]['Feature'][:25]}: {coef_df_nonzero.iloc[2]['Coefficient']:+.4f}
   {coef_df_nonzero.iloc[3]['Feature'][:25]}: {coef_df_nonzero.iloc[3]['Coefficient']:+.4f}
   {coef_df_nonzero.iloc[4]['Feature'][:25]}: {coef_df_nonzero.iloc[4]['Coefficient']:+.4f}

4. 80% ì„¤ëª…ë ¥:
   í•„ìš” ë³€ìˆ˜: {n_features_80}ê°œ
   (ì „ì²´ì˜ {n_features_80/len(feature_cols)*100:.1f}%)

5. ê³„ìˆ˜ ë¶„í¬:
   ì–‘ìˆ˜ ê³„ìˆ˜: {np.sum(coefficients > 0)}ê°œ (ê°€ê²© ìƒìŠ¹)
   ìŒìˆ˜ ê³„ìˆ˜: {np.sum(coefficients < 0)}ê°œ (ê°€ê²© í•˜ë½)

6. SHAP vs ê³„ìˆ˜:
   ìƒê´€ê³„ìˆ˜: {corr:.3f}
   {'â†’ ì¼ì¹˜ë„ ë†’ìŒ' if corr > 0.8 else 'â†’ ì¼ì¹˜ë„ ë³´í†µ' if corr > 0.5 else 'â†’ ì¼ì¹˜ë„ ë‚®ìŒ'}
"""
ax12.text(0.05, 0.95, summary, fontsize=9, verticalalignment='top',
         family='monospace', transform=ax12.transAxes)
ax12.set_title('ì¢…í•© ìš”ì•½', fontsize=14, fontweight='bold', loc='left')

plt.tight_layout()
plt.savefig('elasticnet_xai_analysis.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: elasticnet_xai_analysis.png")

# ========================================
# 7. ê²°ê³¼ ì €ì¥
# ========================================
# ê³„ìˆ˜
coef_df_nonzero.to_csv('elasticnet_coefficients.csv', index=False)
print("âœ… ì €ì¥: elasticnet_coefficients.csv")

# SHAP
shap_df.to_csv('elasticnet_shap_importance.csv', index=False)
print("âœ… ì €ì¥: elasticnet_shap_importance.csv")

# ë¹„êµ
comparison_df.to_csv('elasticnet_coef_shap_comparison.csv', index=False)
print("âœ… ì €ì¥: elasticnet_coef_shap_comparison.csv")

print("\n" + "="*80)
print("XAI ë¶„ì„ ì™„ë£Œ!")
print("="*80)
