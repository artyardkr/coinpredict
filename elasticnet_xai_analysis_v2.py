#!/usr/bin/env python3
"""
ElasticNet XAI V2 (ì„¤ëª…ê°€ëŠ¥í•œ AI) ë¶„ì„ + ê³¼ì í•© ê²€ì¦

V1 (88ê°œ ë³€ìˆ˜) â†’ V2 (138ê°œ ë³€ìˆ˜)

ë¶„ì„ í•­ëª©:
1. ê³„ìˆ˜ (Coefficients) ë¶„ì„
2. Feature Importance
3. SHAP Values
4. ê³¼ì í•© ê²€ì¦ (Learning Curve, Residual ë¶„ì„)
5. ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…
6. V1 vs V2 ë¹„êµ

ì¶œë ¥: 3ê°œì˜ PNG íŒŒì¼ë¡œ ë¶„ë¦¬
- Part 1: ê³¼ì í•© ê²€ì¦
- Part 2: Feature Importance & ê³„ìˆ˜
- Part 3: SHAP ë¶„ì„
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.model_selection import learning_curve
import shap
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    plt.rcParams['font.family'] = 'AppleGothic'
    plt.rcParams['axes.unicode_minus'] = False
except:
    try:
        plt.rcParams['font.family'] = 'Apple SD Gothic Neo'
        plt.rcParams['axes.unicode_minus'] = False
    except:
        print("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. AppleGothic ë˜ëŠ” Apple SD Gothic Neoë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

print("="*80)
print("ElasticNet XAI V2 ë¶„ì„ + ê³¼ì í•© ê²€ì¦")
print("="*80)

# ========================================
# 1. ë°ì´í„° ì¤€ë¹„ ë° ëª¨ë¸ í•™ìŠµ
# ========================================
print("\n[1/7] ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ í•™ìŠµ (V2)...")

df = pd.read_csv('integrated_data_full_v2.csv')
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values('Date').reset_index(drop=True)

print(f"V2 ë°ì´í„°: {df.shape}")

# Target
df['target'] = df['Close'].shift(-1)
df = df[:-1].copy()

# Feature ì„ íƒ
exclude_cols = [
    'Date', 'Close', 'High', 'Low', 'Open', 'target',
    'cumulative_return', 'bc_market_price', 'bc_market_cap',
]

ema_sma_cols = [col for col in df.columns if ('EMA' in col or 'SMA' in col) and 'close' in col.lower()]
exclude_cols.extend(ema_sma_cols)
bb_cols = [col for col in df.columns if col.startswith('BB_')]
exclude_cols.extend(bb_cols)
exclude_cols = list(set(exclude_cols))

feature_cols = [col for col in df.columns if col not in exclude_cols]

for col in feature_cols:
    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

print(f"Features: {len(feature_cols)}ê°œ (V2)")

# ì‹ ê·œ ë³€ìˆ˜ í™•ì¸
new_vars_keywords = ['DXY', 'ETH', 'TLT', 'GLD', 'WALCL', 'RRPONTSYD', 'FED_NET_LIQUIDITY',
                     'NVT', 'Puell', 'Hash_Ribbon', 'IBIT', 'FBTC', 'GBTC_Premium']
new_vars_found = [col for col in feature_cols if any(kw in col for kw in new_vars_keywords)]
print(f"ì‹ ê·œ V2 ë³€ìˆ˜: {len(new_vars_found)}ê°œ ë°œê²¬")

# Train/Test Split (70/30)
split_idx = int(len(df) * 0.7)
split_date = df['Date'].iloc[split_idx]

X_train = df.iloc[:split_idx][feature_cols].values
X_test = df.iloc[split_idx:][feature_cols].values
y_train = df.iloc[:split_idx]['target'].values
y_test = df.iloc[split_idx:]['target'].values

# Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ElasticNet í•™ìŠµ
print("ElasticNet í•™ìŠµ ì¤‘...")
model = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000, random_state=42)
model.fit(X_train_scaled, y_train)

# ì˜ˆì¸¡
y_pred_train = model.predict(X_train_scaled)
y_pred_test = model.predict(X_test_scaled)

# ì„±ëŠ¥
r2_train = r2_score(y_train, y_pred_train)
r2_test = r2_score(y_test, y_pred_test)
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

print(f"\nì„±ëŠ¥ (V2):")
print(f"  Train RÂ²: {r2_train:.4f}, RMSE: ${rmse_train:,.2f}")
print(f"  Test RÂ²:  {r2_test:.4f}, RMSE: ${rmse_test:,.2f}")
print(f"  ì°¨ì´ (Train-Test): {r2_train - r2_test:.4f}")

# ê³¼ì í•© ì§€í‘œ
overfit_score = r2_train - r2_test
if overfit_score < 0.05:
    overfit_status = "âœ… ê³¼ì í•© ì—†ìŒ (ë§¤ìš° ì–‘í˜¸)"
elif overfit_score < 0.15:
    overfit_status = "âš ï¸ ì•½ê°„ì˜ ê³¼ì í•© (í—ˆìš© ê°€ëŠ¥)"
else:
    overfit_status = "âŒ ê³¼ì í•© ì˜ì‹¬ (ì£¼ì˜ í•„ìš”)"

print(f"  ê³¼ì í•© í‰ê°€: {overfit_status}")

# ========================================
# 2. ê³¼ì í•© ê²€ì¦ - Learning Curve
# ========================================
print("\n" + "="*60)
print("[2/7] ê³¼ì í•© ê²€ì¦ - Learning Curve")
print("="*60)

print("Learning Curve ê³„ì‚° ì¤‘...")
train_sizes = np.linspace(0.1, 1.0, 10)
train_sizes_abs, train_scores, test_scores = learning_curve(
    model, X_train_scaled, y_train,
    train_sizes=train_sizes,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

train_scores_mean = train_scores.mean(axis=1)
train_scores_std = train_scores.std(axis=1)
test_scores_mean = test_scores.mean(axis=1)
test_scores_std = test_scores.std(axis=1)

print(f"\nLearning Curve ê²°ê³¼:")
print(f"  ìµœì†Œ ë°ì´í„° (10%): Train={train_scores_mean[0]:.3f}, Test={test_scores_mean[0]:.3f}")
print(f"  ìµœëŒ€ ë°ì´í„° (100%): Train={train_scores_mean[-1]:.3f}, Test={test_scores_mean[-1]:.3f}")
print(f"  Gap (ë§ˆì§€ë§‰): {train_scores_mean[-1] - test_scores_mean[-1]:.3f}")

if train_scores_mean[-1] - test_scores_mean[-1] < 0.1:
    print("  âœ… ë°ì´í„° ì¦ê°€í•´ë„ Gap ì‘ìŒ â†’ ê³¼ì í•© ì—†ìŒ")
else:
    print("  âš ï¸ ë°ì´í„° ì¦ê°€í•´ë„ Gap í¼ â†’ ê³¼ì í•© ê°€ëŠ¥ì„±")

# ========================================
# 3. ê³¼ì í•© ê²€ì¦ - Residual ë¶„ì„
# ========================================
print("\n" + "="*60)
print("[3/7] ê³¼ì í•© ê²€ì¦ - Residual ë¶„ì„")
print("="*60)

# ì”ì°¨
residuals_train = y_train - y_pred_train
residuals_test = y_test - y_pred_test

# ì”ì°¨ í†µê³„
print(f"\nTrain Residuals:")
print(f"  í‰ê· : ${residuals_train.mean():,.2f}")
print(f"  í‘œì¤€í¸ì°¨: ${residuals_train.std():,.2f}")

print(f"\nTest Residuals:")
print(f"  í‰ê· : ${residuals_test.mean():,.2f}")
print(f"  í‘œì¤€í¸ì°¨: ${residuals_test.std():,.2f}")

# ì”ì°¨ íŒ¨í„´ í™•ì¸ (ìê¸°ìƒê´€)
residual_autocorr_train = np.corrcoef(residuals_train[:-1], residuals_train[1:])[0, 1]
residual_autocorr_test = np.corrcoef(residuals_test[:-1], residuals_test[1:])[0, 1]

print(f"\nì”ì°¨ ìê¸°ìƒê´€ (íŒ¨í„´ ê²€ì‚¬):")
print(f"  Train: {residual_autocorr_train:.3f}")
print(f"  Test:  {residual_autocorr_test:.3f}")

if abs(residual_autocorr_test) < 0.2:
    print("  âœ… ì”ì°¨ íŒ¨í„´ ì—†ìŒ (ëœë¤) â†’ ëª¨ë¸ ì–‘í˜¸")
else:
    print("  âš ï¸ ì”ì°¨ì— íŒ¨í„´ ìˆìŒ â†’ ê°œì„  í•„ìš”")

# ========================================
# 4. ê³„ìˆ˜ (Coefficients) ë¶„ì„
# ========================================
print("\n" + "="*60)
print("[4/7] ê³„ìˆ˜ ë¶„ì„ (V2)")
print("="*60)

# ê³„ìˆ˜ ì¶”ì¶œ
coefficients = model.coef_
intercept = model.intercept_

print(f"\nIntercept (ì ˆí¸): ${intercept:,.2f}")
print(f"Non-zero ê³„ìˆ˜: {np.sum(coefficients != 0)}/{len(coefficients)}")

# ê³„ìˆ˜ DataFrame
coef_df = pd.DataFrame({
    'Feature': feature_cols,
    'Coefficient': coefficients,
    'Abs_Coefficient': np.abs(coefficients)
}).sort_values('Abs_Coefficient', ascending=False)

# 0ì´ ì•„ë‹Œ ê³„ìˆ˜ë§Œ
coef_df_nonzero = coef_df[coef_df['Coefficient'] != 0].copy()

print(f"\nTop 10 ì¤‘ìš” ë³€ìˆ˜ (ì ˆëŒ“ê°’ ê¸°ì¤€):")
for i, row in coef_df_nonzero.head(10).iterrows():
    sign = 'ğŸ“ˆ' if row['Coefficient'] > 0 else 'ğŸ“‰'
    is_new = 'ğŸ†•' if any(kw in row['Feature'] for kw in new_vars_keywords) else '  '
    print(f"  {is_new}{sign} {row['Feature']:<35s}: {row['Coefficient']:+.4f}")

print(f"\nì–‘ìˆ˜ ê³„ìˆ˜ (ê°€ê²© ìƒìŠ¹ ìš”ì¸): {np.sum(coefficients > 0)}ê°œ")
print(f"ìŒìˆ˜ ê³„ìˆ˜ (ê°€ê²© í•˜ë½ ìš”ì¸): {np.sum(coefficients < 0)}ê°œ")
print(f"0 ê³„ìˆ˜ (ì œê±°ëœ ë³€ìˆ˜): {np.sum(coefficients == 0)}ê°œ ({np.sum(coefficients == 0)/len(coefficients)*100:.1f}%)")

# ì‹ ê·œ ë³€ìˆ˜ ì¤‘ ì‚¬ìš©ëœ ê°œìˆ˜
new_vars_used = sum(1 for col in new_vars_found if coef_df.loc[coef_df['Feature']==col, 'Coefficient'].values[0] != 0)
print(f"\nì‹ ê·œ V2 ë³€ìˆ˜ ì¤‘ ì‚¬ìš©: {new_vars_used}/{len(new_vars_found)}ê°œ ({new_vars_used/len(new_vars_found)*100:.1f}%)")

# ========================================
# 5. SHAP Values ë¶„ì„
# ========================================
print("\n" + "="*60)
print("[5/7] SHAP Values ë¶„ì„ (V2)")
print("="*60)

print("SHAP Explainer ìƒì„± ì¤‘...")
explainer = shap.LinearExplainer(model, X_train_scaled)

print("SHAP values ê³„ì‚° ì¤‘ (Test set)...")
shap_values_test = explainer.shap_values(X_test_scaled)

# SHAP í‰ê·  ì ˆëŒ“ê°’ (ì „ì—­ ì¤‘ìš”ë„)
shap_importance = np.abs(shap_values_test).mean(axis=0)
shap_df = pd.DataFrame({
    'Feature': feature_cols,
    'SHAP_Importance': shap_importance
}).sort_values('SHAP_Importance', ascending=False)

print(f"\nTop 10 SHAP ì¤‘ìš”ë„:")
for i, row in shap_df.head(10).iterrows():
    is_new = 'ğŸ†•' if any(kw in row['Feature'] for kw in new_vars_keywords) else '  '
    print(f"  {is_new}{row['Feature']:<35s}: {row['SHAP_Importance']:.4f}")

# ì‹ ê·œ ë³€ìˆ˜ì˜ SHAP ì¤‘ìš”ë„
new_vars_shap = shap_df[shap_df['Feature'].isin(new_vars_found)]
print(f"\nì‹ ê·œ V2 ë³€ìˆ˜ SHAP ì¤‘ìš”ë„:")
print(f"  í‰ê· : {new_vars_shap['SHAP_Importance'].mean():.4f}")
print(f"  Top ì‹ ê·œ: {new_vars_shap.iloc[0]['Feature']} ({new_vars_shap.iloc[0]['SHAP_Importance']:.4f})")

# ========================================
# 6. Feature Importance
# ========================================
print("\n" + "="*60)
print("[6/7] Feature Importance")
print("="*60)

feature_importance = np.abs(coefficients)
importance_df = pd.DataFrame({
    'Feature': feature_cols,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

importance_df['Cumulative'] = importance_df['Importance'].cumsum() / importance_df['Importance'].sum()

n_features_80 = (importance_df['Cumulative'] <= 0.8).sum() + 1

print(f"\n80% ì„¤ëª…ë ¥ì„ ìœ„í•œ ë³€ìˆ˜ ê°œìˆ˜: {n_features_80}ê°œ (ì´ {len(feature_cols)}ê°œ ì¤‘ {n_features_80/len(feature_cols)*100:.1f}%)")

# ========================================
# 7. ì‹œê°í™” (3ê°œ íŒŒì¼ë¡œ ë¶„ë¦¬)
# ========================================
print("\n" + "="*60)
print("[7/7] ì‹œê°í™” ìƒì„± ì¤‘...")
print("="*60)

# ==================== Part 1: ê³¼ì í•© ê²€ì¦ ====================
print("\nPart 1: ê³¼ì í•© ê²€ì¦ ì‹œê°í™”...")
fig1 = plt.figure(figsize=(20, 10))

# (1) Train vs Test RÂ²
ax1 = plt.subplot(2, 3, 1)
bars = ax1.bar(['Train', 'Test'], [r2_train, r2_test], color=['steelblue', 'coral'], alpha=0.7)
for bar, val in zip(bars, [r2_train, r2_test]):
    ax1.text(bar.get_x() + bar.get_width()/2, val, f'{val:.3f}',
            ha='center', va='bottom', fontweight='bold', fontsize=12)
ax1.set_ylabel('RÂ² Score', fontweight='bold', fontsize=12)
ax1.set_title(f'Train vs Test RÂ² (Gap: {r2_train-r2_test:.3f})', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='y')
ax1.text(0.5, 0.5, overfit_status, ha='center', va='center',
        transform=ax1.transAxes, fontsize=11, fontweight='bold',
        bbox=dict(boxstyle='round', facecolor='yellow' if 'âš ' in overfit_status else 'lightgreen', alpha=0.7))

# (2) Learning Curve
ax2 = plt.subplot(2, 3, 2)
ax2.plot(train_sizes_abs, train_scores_mean, 'o-', color='steelblue', label='Train', linewidth=2)
ax2.fill_between(train_sizes_abs, train_scores_mean - train_scores_std,
                train_scores_mean + train_scores_std, alpha=0.2, color='steelblue')
ax2.plot(train_sizes_abs, test_scores_mean, 'o-', color='coral', label='Cross-val', linewidth=2)
ax2.fill_between(train_sizes_abs, test_scores_mean - test_scores_std,
                test_scores_mean + test_scores_std, alpha=0.2, color='coral')
ax2.set_xlabel('í•™ìŠµ ë°ì´í„° í¬ê¸°', fontweight='bold', fontsize=12)
ax2.set_ylabel('RÂ² Score', fontweight='bold', fontsize=12)
ax2.set_title('Learning Curve (ê³¼ì í•© ê²€ì‚¬)', fontsize=14, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# (3) Residual Distribution - Train
ax3 = plt.subplot(2, 3, 3)
ax3.hist(residuals_train, bins=50, color='steelblue', alpha=0.7, edgecolor='black')
ax3.axvline(0, color='red', linestyle='--', linewidth=2)
ax3.set_xlabel('Residual ($)', fontweight='bold', fontsize=12)
ax3.set_ylabel('ë¹ˆë„', fontweight='bold', fontsize=12)
ax3.set_title(f'Train Residuals (í‰ê· : ${residuals_train.mean():.0f})', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')

# (4) Residual Distribution - Test
ax4 = plt.subplot(2, 3, 4)
ax4.hist(residuals_test, bins=50, color='coral', alpha=0.7, edgecolor='black')
ax4.axvline(0, color='red', linestyle='--', linewidth=2)
ax4.set_xlabel('Residual ($)', fontweight='bold', fontsize=12)
ax4.set_ylabel('ë¹ˆë„', fontweight='bold', fontsize=12)
ax4.set_title(f'Test Residuals (í‰ê· : ${residuals_test.mean():.0f})', fontsize=14, fontweight='bold')
ax4.grid(True, alpha=0.3, axis='y')

# (5) Residual vs Fitted (Train)
ax5 = plt.subplot(2, 3, 5)
ax5.scatter(y_pred_train, residuals_train, alpha=0.3, s=10, color='steelblue')
ax5.axhline(0, color='red', linestyle='--', linewidth=2)
ax5.set_xlabel('ì˜ˆì¸¡ê°’ ($)', fontweight='bold', fontsize=12)
ax5.set_ylabel('ì”ì°¨ ($)', fontweight='bold', fontsize=12)
ax5.set_title('Residual vs Fitted (Train)', fontsize=14, fontweight='bold')
ax5.grid(True, alpha=0.3)

# (6) Residual vs Fitted (Test)
ax6 = plt.subplot(2, 3, 6)
ax6.scatter(y_pred_test, residuals_test, alpha=0.3, s=10, color='coral')
ax6.axhline(0, color='red', linestyle='--', linewidth=2)
ax6.set_xlabel('ì˜ˆì¸¡ê°’ ($)', fontweight='bold', fontsize=12)
ax6.set_ylabel('ì”ì°¨ ($)', fontweight='bold', fontsize=12)
ax6.set_title('Residual vs Fitted (Test)', fontsize=14, fontweight='bold')
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('elasticnet_xai_v2_part1_overfitting.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: elasticnet_xai_v2_part1_overfitting.png")
plt.close()

# ==================== Part 2: ê³„ìˆ˜ & Feature Importance ====================
print("Part 2: ê³„ìˆ˜ & Feature Importance ì‹œê°í™”...")
fig2 = plt.figure(figsize=(20, 12))

# (1) Top 20 ê³„ìˆ˜
ax1 = plt.subplot(2, 3, 1)
top_coef = coef_df_nonzero.head(20).sort_values('Coefficient')
colors = ['red' if x < 0 else 'green' for x in top_coef['Coefficient']]
ax1.barh(range(len(top_coef)), top_coef['Coefficient'], color=colors, alpha=0.7)
ax1.set_yticks(range(len(top_coef)))
ax1.set_yticklabels(top_coef['Feature'], fontsize=10)
ax1.set_xlabel('ê³„ìˆ˜', fontweight='bold', fontsize=12)
ax1.set_title('Top 20 ë³€ìˆ˜ ê³„ìˆ˜ (V2)', fontsize=14, fontweight='bold')
ax1.axvline(0, color='black', linewidth=1)
ax1.grid(True, alpha=0.3, axis='x')

# (2) ê³„ìˆ˜ ë¶„í¬
ax2 = plt.subplot(2, 3, 2)
ax2.hist(coefficients[coefficients != 0], bins=30, color='steelblue', alpha=0.7, edgecolor='black')
ax2.axvline(0, color='red', linestyle='--', linewidth=2)
ax2.set_xlabel('ê³„ìˆ˜ ê°’', fontweight='bold', fontsize=12)
ax2.set_ylabel('ë¹ˆë„', fontweight='bold', fontsize=12)
ax2.set_title('ê³„ìˆ˜ ë¶„í¬ (Non-zero)', fontsize=14, fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# (3) ì–‘ìˆ˜/ìŒìˆ˜ ê³„ìˆ˜ ë¹„ìœ¨
ax3 = plt.subplot(2, 3, 3)
coef_stats = [
    np.sum(coefficients > 0),
    np.sum(coefficients < 0),
    np.sum(coefficients == 0)
]
labels_coef = ['ì–‘ìˆ˜\n(ìƒìŠ¹)', 'ìŒìˆ˜\n(í•˜ë½)', '0\n(ì œê±°)']
colors_coef = ['green', 'red', 'gray']
bars = ax3.bar(labels_coef, coef_stats, color=colors_coef, alpha=0.7)
for bar, val in zip(bars, coef_stats):
    ax3.text(bar.get_x() + bar.get_width()/2, val, f'{val}ê°œ',
            ha='center', va='bottom', fontweight='bold')
ax3.set_ylabel('ë³€ìˆ˜ ê°œìˆ˜', fontweight='bold', fontsize=12)
ax3.set_title('ê³„ìˆ˜ ë¶€í˜¸ ë¶„í¬ (V2)', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')

# (4) ëˆ„ì  ì¤‘ìš”ë„
ax4 = plt.subplot(2, 3, 4)
ax4.plot(range(1, len(importance_df)+1), importance_df['Cumulative'], linewidth=2, color='purple')
ax4.axhline(0.8, color='red', linestyle='--', linewidth=1, label='80%')
ax4.axvline(n_features_80, color='red', linestyle='--', linewidth=1)
ax4.set_xlabel('ë³€ìˆ˜ ê°œìˆ˜', fontweight='bold', fontsize=12)
ax4.set_ylabel('ëˆ„ì  ì¤‘ìš”ë„', fontweight='bold', fontsize=12)
ax4.set_title('ëˆ„ì  Feature Importance', fontsize=14, fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3)
ax4.text(n_features_80, 0.85, f'{n_features_80}ê°œ', ha='center', fontweight='bold')

# (5) ì‹ ê·œ V2 ë³€ìˆ˜ ì¤‘ìš”ë„
ax5 = plt.subplot(2, 3, 5)
new_vars_top = new_vars_shap.head(15)
if len(new_vars_top) > 0:
    ax5.barh(range(len(new_vars_top)), new_vars_top['SHAP_Importance'], color='orange', alpha=0.7)
    ax5.set_yticks(range(len(new_vars_top)))
    ax5.set_yticklabels(new_vars_top['Feature'], fontsize=10)
    ax5.set_xlabel('SHAP ì¤‘ìš”ë„', fontweight='bold', fontsize=12)
    ax5.set_title('ğŸ†• ì‹ ê·œ V2 ë³€ìˆ˜ Top 15', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='x')

# (6) ìš”ì•½ í…Œì´ë¸”
ax6 = plt.subplot(2, 3, 6)
ax6.axis('off')
summary_data = [
    ['ì§€í‘œ', 'ê°’'],
    ['ëª¨ë¸ ì„±ëŠ¥', ''],
    [f'  Train RÂ²', f'{r2_train:.4f}'],
    [f'  Test RÂ²', f'{r2_test:.4f}'],
    [f'  Gap', f'{r2_train - r2_test:.4f}'],
    ['', ''],
    ['ë³€ìˆ˜ ì„ íƒ', ''],
    [f'  ì´ ë³€ìˆ˜', f'{len(feature_cols)}ê°œ'],
    [f'  ì‚¬ìš© ë³€ìˆ˜', f'{np.sum(coefficients != 0)}ê°œ'],
    [f'  ì œê±° ë³€ìˆ˜', f'{np.sum(coefficients == 0)}ê°œ'],
    ['', ''],
    ['V2 ì‹ ê·œ ë³€ìˆ˜', ''],
    [f'  ì¶”ê°€', f'{len(new_vars_found)}ê°œ'],
    [f'  ì‚¬ìš©', f'{new_vars_used}ê°œ'],
    ['', ''],
    ['80% ì„¤ëª…ë ¥', f'{n_features_80}ê°œ'],
]
table = ax6.table(cellText=summary_data, loc='center', cellLoc='left',
                  colWidths=[0.6, 0.4])
table.auto_set_font_size(False)
table.set_fontsize(11)
table.scale(1, 2.5)
table[(0, 0)].set_facecolor('#4CAF50')
table[(0, 1)].set_facecolor('#4CAF50')
table[(0, 0)].set_text_props(weight='bold', color='white')
table[(0, 1)].set_text_props(weight='bold', color='white')
ax6.set_title('ì¢…í•© ìš”ì•½', fontsize=14, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig('elasticnet_xai_v2_part2_coefficients.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: elasticnet_xai_v2_part2_coefficients.png")
plt.close()

# ==================== Part 3: SHAP ë¶„ì„ ====================
print("Part 3: SHAP ë¶„ì„ ì‹œê°í™”...")
fig3 = plt.figure(figsize=(20, 10))

# (1) SHAP Importance Top 20
ax1 = plt.subplot(1, 3, 1)
top_shap = shap_df.head(20)
ax1.barh(range(len(top_shap)), top_shap['SHAP_Importance'], color='coral', alpha=0.7)
ax1.set_yticks(range(len(top_shap)))
ax1.set_yticklabels(top_shap['Feature'], fontsize=10)
ax1.set_xlabel('í‰ê·  |SHAP Value|', fontweight='bold', fontsize=12)
ax1.set_title('Top 20 SHAP Importance (V2)', fontsize=14, fontweight='bold')
ax1.grid(True, alpha=0.3, axis='x')

# (2) SHAP Summary Plot
ax2 = plt.subplot(1, 3, 2)
plt.sca(ax2)
shap.summary_plot(shap_values_test, X_test_scaled, feature_names=feature_cols,
                  show=False, max_display=20, plot_size=None)
plt.title('SHAP Summary Plot (Top 20, V2)', fontsize=14, fontweight='bold')

# (3) ê³„ìˆ˜ vs SHAP ë¹„êµ
ax3 = plt.subplot(1, 3, 3)
comparison_df = coef_df.merge(shap_df, on='Feature')
comparison_df = comparison_df[comparison_df['Abs_Coefficient'] > 0]
ax3.scatter(comparison_df['Abs_Coefficient'], comparison_df['SHAP_Importance'],
            alpha=0.5, s=50, color='purple')
ax3.set_xlabel('ê³„ìˆ˜ ì ˆëŒ“ê°’', fontweight='bold', fontsize=12)
ax3.set_ylabel('SHAP ì¤‘ìš”ë„', fontweight='bold', fontsize=12)
ax3.set_title('ê³„ìˆ˜ vs SHAP ì¤‘ìš”ë„', fontsize=14, fontweight='bold')
ax3.grid(True, alpha=0.3)

corr = comparison_df['Abs_Coefficient'].corr(comparison_df['SHAP_Importance'])
ax3.text(0.05, 0.95, f'ìƒê´€ê³„ìˆ˜: {corr:.3f}', transform=ax3.transAxes,
         fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.savefig('elasticnet_xai_v2_part3_shap.png', dpi=300, bbox_inches='tight')
print("âœ… ì €ì¥: elasticnet_xai_v2_part3_shap.png")
plt.close()

# ========================================
# 8. ê²°ê³¼ ì €ì¥
# ========================================
# ê³„ìˆ˜
coef_df_nonzero.to_csv('elasticnet_coefficients_v2.csv', index=False)
print("âœ… ì €ì¥: elasticnet_coefficients_v2.csv")

# SHAP
shap_df.to_csv('elasticnet_shap_importance_v2.csv', index=False)
print("âœ… ì €ì¥: elasticnet_shap_importance_v2.csv")

# ë¹„êµ
comparison_df.to_csv('elasticnet_coef_shap_comparison_v2.csv', index=False)
print("âœ… ì €ì¥: elasticnet_coef_shap_comparison_v2.csv")

# ì‹ ê·œ ë³€ìˆ˜
new_vars_shap.to_csv('elasticnet_new_vars_importance_v2.csv', index=False)
print("âœ… ì €ì¥: elasticnet_new_vars_importance_v2.csv")

print("\n" + "="*80)
print("XAI V2 ë¶„ì„ ì™„ë£Œ!")
print("="*80)

print(f"\nğŸ¯ ìµœì¢… í‰ê°€:")
print(f"  ê³¼ì í•©: {overfit_status}")
print(f"  V2 ë³€ìˆ˜ ê¸°ì—¬: {new_vars_used}/{len(new_vars_found)}ê°œ ì‚¬ìš©")
print(f"  ëª¨ë¸ í’ˆì§ˆ: {'âœ… ìš°ìˆ˜' if r2_test > 0.85 and overfit_score < 0.15 else 'âš ï¸ ì–‘í˜¸' if r2_test > 0.7 else 'âŒ ê°œì„  í•„ìš”'}")

print("\nğŸ“Š ì¶œë ¥ íŒŒì¼:")
print("  1. elasticnet_xai_v2_part1_overfitting.png - ê³¼ì í•© ê²€ì¦")
print("  2. elasticnet_xai_v2_part2_coefficients.png - ê³„ìˆ˜ & Feature Importance")
print("  3. elasticnet_xai_v2_part3_shap.png - SHAP ë¶„ì„")
