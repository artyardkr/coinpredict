import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("ëª¨ë¸ í›ˆë ¨: ê¸°ìˆ ì  ì§€í‘œ ì œê±° (2024ë…„~ìµœì‹ , 7:3)")
print("=" * 70)

# ===== 1. ë°ì´í„° ë¡œë“œ =====
print("\n1. ë°ì´í„° ë¡œë“œ ë° íŠ¹ì„± ì„ íƒ")
print("-" * 70)

df = pd.read_csv('integrated_data_full.csv', index_col=0, parse_dates=True)
print(f"ì „ì²´ ë°ì´í„°: {df.shape}")

# 2024ë…„ ì´í›„ ë°ì´í„°
df_2024 = df[df.index >= '2024-01-01'].copy()
print(f"2024ë…„ ì´í›„: {df_2024.shape} ({df_2024.index[0].date()} ~ {df_2024.index[-1].date()})")

# íƒ€ê²Ÿ ì„¤ì •
df_2024['target'] = df_2024['Close'].shift(-1)
df_2024 = df_2024.dropna(subset=['target'])

# ===== 2. ì œê±°í•  íŠ¹ì„± ì •ì˜ =====
print("\n2. ì œê±°í•  íŠ¹ì„± ì •ì˜")
print("-" * 70)

# Closeì™€ ê±°ì˜ ê°™ì€ ê°’ë“¤
exclude_similar_to_close = [
    'Close', 'target',
    'High', 'Low', 'Open',  # ê°™ì€ ë‚  ê°€ê²©
    'cumulative_return',  # Closeë¡œë¶€í„° ì§ì ‘ ê³„ì‚°
    'bc_market_price',  # Closeì™€ ê±°ì˜ ë™ì¼
    'bc_market_cap',  # Close * Supply
]

# ê¸°ìˆ ì  ì§€í‘œ (ëª¨ë‘ ì œê±°)
exclude_technical = [
    # ì´ë™í‰ê· 
    col for col in df_2024.columns if 'EMA' in col or 'SMA' in col
] + [
    # ëª¨ë©˜í…€ ì§€í‘œ
    'RSI', 'Stoch_K', 'Stoch_D', 'Williams_R', 'ROC', 'MFI',
    # íŠ¸ë Œë“œ ì§€í‘œ
    'MACD', 'MACD_signal', 'MACD_diff', 'ADX', 'CCI',
    # ë³€ë™ì„± ì§€í‘œ
    'BB_high', 'BB_low', 'BB_mid', 'BB_width', 'ATR', 'volatility_20d',
    # ê±°ë˜ëŸ‰ ì§€í‘œ
    'OBV', 'volume_change',
    # ìˆ˜ìµë¥ 
    'daily_return',
    # ì‹œê°€ì´ì•¡ (Close ê¸°ë°˜)
    'market_cap_approx',
]

# ì „ì²´ ì œì™¸ ëª©ë¡
exclude_cols = list(set(exclude_similar_to_close + exclude_technical))

print(f"ì œê±°í•  íŠ¹ì„± ìˆ˜: {len(exclude_cols)}ê°œ")
print("\nì œê±° ì¹´í…Œê³ ë¦¬:")
print(f"  - Close ìœ ì‚¬ íŠ¹ì„±: {len(exclude_similar_to_close)}ê°œ")
print(f"  - ê¸°ìˆ ì  ì§€í‘œ: {len([c for c in exclude_technical if c in df_2024.columns])}ê°œ")

# ë‚¨ì€ íŠ¹ì„±
all_features = [col for col in df_2024.columns if col not in exclude_cols]

print(f"\në‚¨ì€ íŠ¹ì„±: {len(all_features)}ê°œ")

# ì¹´í…Œê³ ë¦¬ë³„ íŠ¹ì„± í™•ì¸
categories = {
    'ì›ë³¸ ë°ì´í„°': [c for c in all_features if c in ['Volume', 'Dividends', 'Stock Splits']],
    'ì „í†µ ì‹œì¥': [c for c in all_features if c in ['QQQ', 'SPX', 'UUP', 'EURUSD', 'GOLD', 'SILVER', 'OIL', 'BSV']],
    'ê±°ì‹œê²½ì œ': [c for c in all_features if c in ['DGS10', 'DFF', 'CPIAUCSL', 'UNRATE', 'M2SL', 'GDP', 'DEXUSEU', 'DTWEXBGS', 'T10Y2Y', 'VIXCLS']],
    'ê°ì •/ê´€ì‹¬': [c for c in all_features if 'fear_greed' in c or 'google_trends' in c],
    'ì˜¨ì²´ì¸': [c for c in all_features if c.startswith('bc_') or c.startswith('cm_') or c.startswith('gn_')],
}

print("\në‚¨ì€ íŠ¹ì„± ì¹´í…Œê³ ë¦¬:")
for cat, feats in categories.items():
    if feats:
        print(f"  {cat}: {len(feats)}ê°œ")
        for feat in feats:
            print(f"    - {feat}")

X = df_2024[all_features].copy()
y = df_2024['target'].copy()

print(f"\nìµœì¢… ë°ì´í„°: {X.shape}")
print(f"ìƒ˜í”Œ: {len(X)}ê°œ")

# ===== 3. 7:3 ë¶„í•  =====
print("\n3. ë°ì´í„° ë¶„í•  (7:3)")
print("-" * 70)

split_idx = int(len(X) * 0.7)

X_train = X.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_train = y.iloc[:split_idx]
y_test = y.iloc[split_idx:]

print(f"í›ˆë ¨: {X_train.shape[0]}ê°œ ({X_train.index[0].date()} ~ {X_train.index[-1].date()})")
print(f"í…ŒìŠ¤íŠ¸: {X_test.shape[0]}ê°œ ({X_test.index[0].date()} ~ {X_test.index[-1].date()})")

# ===== 4. ëª¨ë¸ í›ˆë ¨ =====
print("\n" + "=" * 70)
print("4. ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€")
print("=" * 70)

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

all_results = []
predictions = {}

# ===== Random Forest =====
print("\n[Random Forest]")
rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train_scaled, y_train)
y_train_pred_rf = rf.predict(X_train_scaled)
y_test_pred_rf = rf.predict(X_test_scaled)

rf_results = {
    'model': 'Random Forest',
    'train_r2': r2_score(y_train, y_train_pred_rf),
    'test_r2': r2_score(y_test, y_test_pred_rf),
    'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred_rf)),
    'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred_rf)),
    'train_mae': mean_absolute_error(y_train, y_train_pred_rf),
    'test_mae': mean_absolute_error(y_test, y_test_pred_rf),
    'train_mape': np.mean(np.abs((y_train - y_train_pred_rf) / y_train)) * 100,
    'test_mape': np.mean(np.abs((y_test - y_test_pred_rf) / y_test)) * 100,
}

all_results.append(rf_results)
predictions['RF'] = y_test_pred_rf

print(f"  Train RÂ²: {rf_results['train_r2']:.4f} | Test RÂ²: {rf_results['test_r2']:.4f}")
print(f"  Train RMSE: ${rf_results['train_rmse']:,.2f} | Test RMSE: ${rf_results['test_rmse']:,.2f}")
print(f"  Train MAE: ${rf_results['train_mae']:,.2f} | Test MAE: ${rf_results['test_mae']:,.2f}")
print(f"  Train MAPE: {rf_results['train_mape']:.2f}% | Test MAPE: {rf_results['test_mape']:.2f}%")

# Feature Importance (Random Forest)
rf_importance = pd.DataFrame({
    'feature': all_features,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\n  ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
for i, row in rf_importance.head(10).iterrows():
    print(f"    {i+1:2}. {row['feature']:30} : {row['importance']:.4f}")

# ===== XGBoost =====
print("\n[XGBoost]")
xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train_scaled, y_train)
y_train_pred_xgb = xgb_model.predict(X_train_scaled)
y_test_pred_xgb = xgb_model.predict(X_test_scaled)

xgb_results = {
    'model': 'XGBoost',
    'train_r2': r2_score(y_train, y_train_pred_xgb),
    'test_r2': r2_score(y_test, y_test_pred_xgb),
    'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred_xgb)),
    'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred_xgb)),
    'train_mae': mean_absolute_error(y_train, y_train_pred_xgb),
    'test_mae': mean_absolute_error(y_test, y_test_pred_xgb),
    'train_mape': np.mean(np.abs((y_train - y_train_pred_xgb) / y_train)) * 100,
    'test_mape': np.mean(np.abs((y_test - y_test_pred_xgb) / y_test)) * 100,
}

all_results.append(xgb_results)
predictions['XGB'] = y_test_pred_xgb

print(f"  Train RÂ²: {xgb_results['train_r2']:.4f} | Test RÂ²: {xgb_results['test_r2']:.4f}")
print(f"  Train RMSE: ${xgb_results['train_rmse']:,.2f} | Test RMSE: ${xgb_results['test_rmse']:,.2f}")
print(f"  Train MAE: ${xgb_results['train_mae']:,.2f} | Test MAE: ${xgb_results['test_mae']:,.2f}")
print(f"  Train MAPE: {xgb_results['train_mape']:.2f}% | Test MAPE: {xgb_results['test_mape']:.2f}%")

# Feature Importance (XGBoost)
xgb_importance = pd.DataFrame({
    'feature': all_features,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\n  ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
for i, row in xgb_importance.head(10).iterrows():
    print(f"    {i+1:2}. {row['feature']:30} : {row['importance']:.4f}")

# ===== 5. ê²°ê³¼ ì €ì¥ =====
print("\n" + "=" * 70)
print("5. ê²°ê³¼ ì €ì¥")
print("=" * 70)

results_df = pd.DataFrame(all_results)
results_df.to_csv('model_results_no_technical.csv', index=False)
print("âœ“ model_results_no_technical.csv")

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸
best_model = results_df.loc[results_df['test_r2'].idxmax()]
print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_model['model']}")
print(f"   Test RÂ²: {best_model['test_r2']:.4f}")
print(f"   Test RMSE: ${best_model['test_rmse']:,.2f}")
print(f"   Test MAPE: {best_model['test_mape']:.2f}%")

# ===== 6. ì‹œê°í™” =====
print("\n6. ê²°ê³¼ ì‹œê°í™”")
print("-" * 70)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. ëª¨ë¸ ë¹„êµ (RÂ²)
models = ['Random Forest', 'XGBoost']
train_r2 = [rf_results['train_r2'], xgb_results['train_r2']]
test_r2 = [rf_results['test_r2'], xgb_results['test_r2']]

x_pos = np.arange(len(models))
width = 0.35

axes[0, 0].bar(x_pos - width/2, train_r2, width, label='Train RÂ²', alpha=0.8)
axes[0, 0].bar(x_pos + width/2, test_r2, width, label='Test RÂ²', alpha=0.8)
axes[0, 0].set_xlabel('Model')
axes[0, 0].set_ylabel('RÂ² Score')
axes[0, 0].set_title('Model Comparison (RÂ²)', fontweight='bold')
axes[0, 0].set_xticks(x_pos)
axes[0, 0].set_xticklabels(models)
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3, axis='y')
axes[0, 0].axhline(y=0, color='r', linestyle='--', alpha=0.5)

# 2. ëª¨ë¸ ë¹„êµ (RMSE & MAPE)
ax2 = axes[0, 1]
ax2_twin = ax2.twinx()

test_rmse = [rf_results['test_rmse'], xgb_results['test_rmse']]
test_mape = [rf_results['test_mape'], xgb_results['test_mape']]

color1 = 'tab:blue'
ax2.bar(x_pos - width/2, test_rmse, width, label='Test RMSE', color=color1, alpha=0.8)
ax2.set_xlabel('Model')
ax2.set_ylabel('RMSE ($)', color=color1)
ax2.tick_params(axis='y', labelcolor=color1)
ax2.set_xticks(x_pos)
ax2.set_xticklabels(models)

color2 = 'tab:orange'
ax2_twin.bar(x_pos + width/2, test_mape, width, label='Test MAPE', color=color2, alpha=0.8)
ax2_twin.set_ylabel('MAPE (%)', color=color2)
ax2_twin.tick_params(axis='y', labelcolor=color2)

axes[0, 1].set_title('Model Comparison (RMSE & MAPE)', fontweight='bold')
ax2.grid(True, alpha=0.3, axis='y')

# 3. Feature Importance ë¹„êµ (ìƒìœ„ 15ê°œ)
top_rf = rf_importance.head(15)
top_xgb = xgb_importance.head(15)

axes[1, 0].barh(range(len(top_rf)), top_rf['importance'], alpha=0.7)
axes[1, 0].set_yticks(range(len(top_rf)))
axes[1, 0].set_yticklabels(top_rf['feature'], fontsize=8)
axes[1, 0].set_xlabel('Importance')
axes[1, 0].set_title('Random Forest: Top 15 Features', fontweight='bold')
axes[1, 0].invert_yaxis()
axes[1, 0].grid(True, alpha=0.3, axis='x')

axes[1, 1].barh(range(len(top_xgb)), top_xgb['importance'], alpha=0.7, color='orange')
axes[1, 1].set_yticks(range(len(top_xgb)))
axes[1, 1].set_yticklabels(top_xgb['feature'], fontsize=8)
axes[1, 1].set_xlabel('Importance')
axes[1, 1].set_title('XGBoost: Top 15 Features', fontweight='bold')
axes[1, 1].invert_yaxis()
axes[1, 1].grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('model_no_technical_comparison.png', dpi=300, bbox_inches='tight')
print("âœ“ model_no_technical_comparison.png")

# ì‹œê³„ì—´ ì˜ˆì¸¡ í”Œë¡¯
fig, axes = plt.subplots(2, 1, figsize=(14, 10))

test_dates = y_test.index

# Random Forest
axes[0].plot(test_dates, y_test.values, label='Actual', linewidth=2, color='blue')
axes[0].plot(test_dates, y_test_pred_rf, label='Predicted', linewidth=2,
            color='red', alpha=0.7, linestyle='--')
axes[0].set_xlabel('Date', fontsize=11)
axes[0].set_ylabel('BTC Price ($)', fontsize=11)
axes[0].set_title(f'Random Forest: Time Series Prediction (RÂ²={rf_results["test_r2"]:.4f})',
                 fontsize=12, fontweight='bold')
axes[0].legend()
axes[0].grid(True, alpha=0.3)
axes[0].tick_params(axis='x', rotation=45)

# XGBoost
axes[1].plot(test_dates, y_test.values, label='Actual', linewidth=2, color='blue')
axes[1].plot(test_dates, y_test_pred_xgb, label='Predicted', linewidth=2,
            color='red', alpha=0.7, linestyle='--')
axes[1].set_xlabel('Date', fontsize=11)
axes[1].set_ylabel('BTC Price ($)', fontsize=11)
axes[1].set_title(f'XGBoost: Time Series Prediction (RÂ²={xgb_results["test_r2"]:.4f})',
                 fontsize=12, fontweight='bold')
axes[1].legend()
axes[1].grid(True, alpha=0.3)
axes[1].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.savefig('model_no_technical_timeseries.png', dpi=300, bbox_inches='tight')
print("âœ“ model_no_technical_timeseries.png")

plt.close('all')

# ===== 7. 2021ë…„ ê²°ê³¼ì™€ ë¹„êµ =====
print("\n" + "=" * 70)
print("7. ê¸°ìˆ ì  ì§€í‘œ í¬í•¨ vs ì œì™¸ ë¹„êµ")
print("=" * 70)

print("\n2021ë…„ ë°ì´í„° (ê¸°ìˆ ì  ì§€í‘œ í¬í•¨):")
results_2021 = pd.read_csv('model_results_2021.csv')
best_2021 = results_2021.iloc[0]
print(f"  ëª¨ë¸: {best_2021['model']}")
print(f"  Test RÂ²: {best_2021['test_r2']:.4f}")
print(f"  Test RMSE: ${best_2021['test_rmse']:,.2f}")
print(f"  Test MAPE: {best_2021['test_mape']:.2f}%")

print(f"\n2024ë…„ ë°ì´í„° (ê¸°ìˆ ì  ì§€í‘œ ì œì™¸):")
print(f"  ëª¨ë¸: {best_model['model']}")
print(f"  Test RÂ²: {best_model['test_r2']:.4f}")
print(f"  Test RMSE: ${best_model['test_rmse']:,.2f}")
print(f"  Test MAPE: {best_model['test_mape']:.2f}%")

print("\n" + "=" * 70)
print("ê¸°ìˆ ì  ì§€í‘œ ì œê±° ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
print("=" * 70)
