import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("Phase 4: ë‹¤ì¤‘ ì˜ˆì¸¡ ê¸°ê°„ ëª¨ë¸ (7ì¼, 30ì¼)")
print("=" * 70)

# ===== 1. ë°ì´í„° ë¡œë“œ =====
print("\n1. ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§")
print("-" * 70)

df = pd.read_csv('integrated_data_full.csv', index_col=0, parse_dates=True)
print(f"ì „ì²´ ë°ì´í„°: {df.shape} ({df.index[0].date()} ~ {df.index[-1].date()})")

# 2021ë…„ ë°ì´í„°ë§Œ í•„í„°ë§
df_2021 = df[df.index.year == 2021].copy()
print(f"2021ë…„ ë°ì´í„°: {df_2021.shape} ({df_2021.index[0].date()} ~ {df_2021.index[-1].date()})")

# ì œê±°í•  íŠ¹ì„±
exclude_cols = ['Close', 'cumulative_return', 'High', 'Low', 'Open',
                'bc_market_price', 'bc_market_cap']
all_features = [col for col in df_2021.columns if col not in exclude_cols]

print(f"íŠ¹ì„±: {len(all_features)}ê°œ")

# ===== 2. ë‹¤ì¤‘ íƒ€ê²Ÿ ìƒì„± =====
print("\n2. ë‹¤ì¤‘ ì˜ˆì¸¡ ê¸°ê°„ íƒ€ê²Ÿ ìƒì„±")
print("-" * 70)

# 1ì¼, 7ì¼, 30ì¼ í›„ ì¢…ê°€
df_2021['target_1d'] = df_2021['Close'].shift(-1)
df_2021['target_7d'] = df_2021['Close'].shift(-7)
df_2021['target_30d'] = df_2021['Close'].shift(-30)

# ê° íƒ€ê²Ÿë³„ ìœ íš¨í•œ ë°ì´í„°
df_1d = df_2021.dropna(subset=['target_1d']).copy()
df_7d = df_2021.dropna(subset=['target_7d']).copy()
df_30d = df_2021.dropna(subset=['target_30d']).copy()

print(f"1ì¼ ì˜ˆì¸¡: {len(df_1d)}ê°œ ìƒ˜í”Œ (ë§ˆì§€ë§‰ 1ì¼ ì œì™¸)")
print(f"7ì¼ ì˜ˆì¸¡: {len(df_7d)}ê°œ ìƒ˜í”Œ (ë§ˆì§€ë§‰ 7ì¼ ì œì™¸)")
print(f"30ì¼ ì˜ˆì¸¡: {len(df_30d)}ê°œ ìƒ˜í”Œ (ë§ˆì§€ë§‰ 30ì¼ ì œì™¸)")

# ===== 3. íŠ¹ì„± ì§‘í•© ë¡œë“œ =====
print("\n3. íŠ¹ì„± ì§‘í•© ë¡œë“œ")
print("-" * 70)

feature_sets = {}

for n in [10, 20, 30, 40, 50]:
    with open(f'selected_features_top{n}.txt', 'r') as f:
        lines = f.readlines()
        features = [line.strip().split('. ')[1] for line in lines if line.strip() and '. ' in line]
        feature_sets[f'top{n}'] = features
        print(f"âœ“ Top {n}: {len(features)}ê°œ")

feature_sets['all'] = all_features
print(f"âœ“ All: {len(all_features)}ê°œ")

# ===== 4. ëª¨ë¸ í›ˆë ¨ í•¨ìˆ˜ =====
def train_and_evaluate_horizon(df_horizon, target_col, horizon_name, features, model_name, model):
    """íŠ¹ì • ì˜ˆì¸¡ ê¸°ê°„ì— ëŒ€í•œ ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€"""

    X = df_horizon[features].copy()
    y = df_horizon[target_col].copy()

    # 7:3 ë¶„í• 
    split_idx = int(len(X) * 0.7)
    X_train = X.iloc[:split_idx]
    X_test = X.iloc[split_idx:]
    y_train = y.iloc[:split_idx]
    y_test = y.iloc[split_idx:]

    # ìŠ¤ì¼€ì¼ë§
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # í›ˆë ¨
    model.fit(X_train_scaled, y_train)

    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)

    # í‰ê°€
    results = {
        'horizon': horizon_name,
        'model': model_name,
        'train_r2': r2_score(y_train, y_train_pred),
        'test_r2': r2_score(y_test, y_test_pred),
        'train_rmse': np.sqrt(mean_squared_error(y_train, y_train_pred)),
        'test_rmse': np.sqrt(mean_squared_error(y_test, y_test_pred)),
        'train_mae': mean_absolute_error(y_train, y_train_pred),
        'test_mae': mean_absolute_error(y_test, y_test_pred),
        'train_mape': np.mean(np.abs((y_train - y_train_pred) / y_train)) * 100,
        'test_mape': np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100,
    }

    return results, y_test, y_test_pred, X_test.index

# ===== 5. ëª¨ë“  ì¡°í•© í›ˆë ¨ =====
print("\n" + "=" * 70)
print("4. ë‹¤ì¤‘ ê¸°ê°„ ëª¨ë¸ í›ˆë ¨ (1ì¼, 7ì¼, 30ì¼)")
print("=" * 70)

all_results = []
all_predictions = {}

horizons = [
    (df_1d, 'target_1d', '1-day', 1),
    (df_7d, 'target_7d', '7-day', 7),
    (df_30d, 'target_30d', '30-day', 30)
]

for df_horizon, target_col, horizon_name, days in horizons:
    print(f"\n{'='*70}")
    print(f"ì˜ˆì¸¡ ê¸°ê°„: {horizon_name} ({len(df_horizon)}ê°œ ìƒ˜í”Œ)")
    print(f"{'='*70}")

    for feature_name, features in feature_sets.items():
        print(f"\níŠ¹ì„± ì§‘í•©: {feature_name} ({len(features)}ê°œ)")
        print("-" * 70)

        # Random Forest
        rf = RandomForestRegressor(
            n_estimators=100,
            max_depth=5,
            min_samples_split=20,
            min_samples_leaf=10,
            random_state=42,
            n_jobs=-1
        )

        rf_results, y_test, rf_pred, test_dates = train_and_evaluate_horizon(
            df_horizon, target_col, horizon_name, features,
            f'RF_{feature_name}', rf
        )
        rf_results['feature_set'] = feature_name
        rf_results['n_features'] = len(features)
        rf_results['days'] = days
        all_results.append(rf_results)
        all_predictions[f'{horizon_name}_RF_{feature_name}'] = (y_test, rf_pred, test_dates)

        print(f"  [RF] Test RÂ²: {rf_results['test_r2']:.4f} | RMSE: ${rf_results['test_rmse']:,.2f} | MAPE: {rf_results['test_mape']:.2f}%")

        # XGBoost
        xgb_model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=3,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )

        xgb_results, y_test, xgb_pred, test_dates = train_and_evaluate_horizon(
            df_horizon, target_col, horizon_name, features,
            f'XGB_{feature_name}', xgb_model
        )
        xgb_results['feature_set'] = feature_name
        xgb_results['n_features'] = len(features)
        xgb_results['days'] = days
        all_results.append(xgb_results)
        all_predictions[f'{horizon_name}_XGB_{feature_name}'] = (y_test, xgb_pred, test_dates)

        print(f"  [XGB] Test RÂ²: {xgb_results['test_r2']:.4f} | RMSE: ${xgb_results['test_rmse']:,.2f} | MAPE: {xgb_results['test_mape']:.2f}%")

# ===== 6. ê²°ê³¼ ì €ì¥ ë° ë¶„ì„ =====
print("\n" + "=" * 70)
print("5. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥")
print("=" * 70)

results_df = pd.DataFrame(all_results)
results_df.to_csv('model_results_multi_horizon.csv', index=False)
print("âœ“ model_results_multi_horizon.csv")

# ê° ì˜ˆì¸¡ ê¸°ê°„ë³„ ìµœê³  ì„±ëŠ¥
print("\nì˜ˆì¸¡ ê¸°ê°„ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
print("-" * 70)
for horizon in ['1-day', '7-day', '30-day']:
    horizon_results = results_df[results_df['horizon'] == horizon].sort_values('test_r2', ascending=False)
    best = horizon_results.iloc[0]
    print(f"\n{horizon} ì˜ˆì¸¡:")
    print(f"  ìµœê³  ëª¨ë¸: {best['model']}")
    print(f"  Test RÂ²: {best['test_r2']:.4f}")
    print(f"  Test RMSE: ${best['test_rmse']:,.2f}")
    print(f"  Test MAPE: {best['test_mape']:.2f}%")
    print(f"  íŠ¹ì„± ìˆ˜: {int(best['n_features'])}ê°œ")

    # ìƒìœ„ 3ê°œ ëª¨ë¸
    print(f"\n  ìƒìœ„ 3ê°œ ëª¨ë¸:")
    for i, row in horizon_results.head(3).iterrows():
        print(f"    {i+1}. {row['model']:20} | RÂ²: {row['test_r2']:7.4f} | RMSE: ${row['test_rmse']:8,.2f} | MAPE: {row['test_mape']:5.2f}%")

# ===== 7. ë¹„êµ ì‹œê°í™” =====
print("\n6. ê²°ê³¼ ì‹œê°í™”")
print("-" * 70)

fig = plt.figure(figsize=(20, 14))
gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

# 1. ì˜ˆì¸¡ ê¸°ê°„ë³„ RÂ² ë¹„êµ (íŠ¹ì„± ìˆ˜ë³„)
ax1 = fig.add_subplot(gs[0, 0])
for horizon in ['1-day', '7-day', '30-day']:
    horizon_data = results_df[results_df['horizon'] == horizon]
    for model_type in ['RF', 'XGB']:
        model_data = horizon_data[horizon_data['model'].str.contains(model_type)]
        ax1.plot(model_data['n_features'], model_data['test_r2'],
                marker='o', label=f'{horizon} {model_type}', linewidth=2)

ax1.set_xlabel('Number of Features', fontsize=11)
ax1.set_ylabel('Test RÂ² Score', fontsize=11)
ax1.set_title('Test RÂ² vs Features (Multi-Horizon)', fontsize=12, fontweight='bold')
ax1.legend(fontsize=8, ncol=2)
ax1.grid(True, alpha=0.3)
ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)

# 2. ì˜ˆì¸¡ ê¸°ê°„ë³„ RMSE ë¹„êµ
ax2 = fig.add_subplot(gs[0, 1])
for horizon in ['1-day', '7-day', '30-day']:
    horizon_data = results_df[results_df['horizon'] == horizon]
    xgb_data = horizon_data[horizon_data['model'].str.contains('XGB')]
    ax2.plot(xgb_data['n_features'], xgb_data['test_rmse'],
            marker='s', label=f'{horizon}', linewidth=2)

ax2.set_xlabel('Number of Features', fontsize=11)
ax2.set_ylabel('Test RMSE ($)', fontsize=11)
ax2.set_title('Test RMSE vs Features (XGBoost)', fontsize=12, fontweight='bold')
ax2.legend(fontsize=9)
ax2.grid(True, alpha=0.3)

# 3. ì˜ˆì¸¡ ê¸°ê°„ë³„ MAPE ë¹„êµ
ax3 = fig.add_subplot(gs[0, 2])
for horizon in ['1-day', '7-day', '30-day']:
    horizon_data = results_df[results_df['horizon'] == horizon]
    xgb_data = horizon_data[horizon_data['model'].str.contains('XGB')]
    ax3.plot(xgb_data['n_features'], xgb_data['test_mape'],
            marker='d', label=f'{horizon}', linewidth=2)

ax3.set_xlabel('Number of Features', fontsize=11)
ax3.set_ylabel('Test MAPE (%)', fontsize=11)
ax3.set_title('Test MAPE vs Features (XGBoost)', fontsize=12, fontweight='bold')
ax3.legend(fontsize=9)
ax3.grid(True, alpha=0.3)

# 4-6. ê° ê¸°ê°„ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼
for idx, horizon in enumerate(['1-day', '7-day', '30-day']):
    ax = fig.add_subplot(gs[1, idx])
    horizon_results = results_df[results_df['horizon'] == horizon].sort_values('test_r2', ascending=False)
    best = horizon_results.iloc[0]

    y_test, y_pred, test_dates = all_predictions[f'{horizon}_{best["model"]}']

    ax.scatter(y_test, y_pred, alpha=0.5, s=30)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
           'r--', linewidth=2, label='Perfect Prediction')
    ax.set_xlabel('Actual Price ($)', fontsize=11)
    ax.set_ylabel('Predicted Price ($)', fontsize=11)
    ax.set_title(f'{horizon} Prediction\n{best["model"]} (RÂ²={best["test_r2"]:.4f})',
                fontsize=11, fontweight='bold')
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)

# 7-9. ê° ê¸°ê°„ë³„ ì‹œê³„ì—´ ì˜ˆì¸¡
for idx, horizon in enumerate(['1-day', '7-day', '30-day']):
    ax = fig.add_subplot(gs[2, idx])
    horizon_results = results_df[results_df['horizon'] == horizon].sort_values('test_r2', ascending=False)
    best = horizon_results.iloc[0]

    y_test, y_pred, test_dates = all_predictions[f'{horizon}_{best["model"]}']

    ax.plot(test_dates, y_test.values, label='Actual', linewidth=2, color='blue')
    ax.plot(test_dates, y_pred, label='Predicted', linewidth=2,
           color='red', alpha=0.7, linestyle='--')
    ax.set_xlabel('Date', fontsize=11)
    ax.set_ylabel('BTC Price ($)', fontsize=11)
    ax.set_title(f'{horizon} Time Series\n{best["model"]} (MAPE={best["test_mape"]:.2f}%)',
                fontsize=11, fontweight='bold')
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)
    ax.tick_params(axis='x', rotation=45)

plt.savefig('multi_horizon_prediction_results.png', dpi=300, bbox_inches='tight')
print("âœ“ multi_horizon_prediction_results.png")

plt.close()

# ===== 8. ì˜ˆì¸¡ ê¸°ê°„ë³„ ì„±ëŠ¥ í•˜ë½ ë¶„ì„ =====
print("\n7. ì˜ˆì¸¡ ê¸°ê°„ë³„ ì„±ëŠ¥ ë³€í™” ë¶„ì„")
print("-" * 70)

# XGBoost Top 10 ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
print("\nXGBoost + Top 10 íŠ¹ì„± ê¸°ì¤€:")
print("-" * 70)

for horizon in ['1-day', '7-day', '30-day']:
    row = results_df[(results_df['horizon'] == horizon) &
                     (results_df['model'] == 'XGB_top10')].iloc[0]
    print(f"\n{horizon} ì˜ˆì¸¡:")
    print(f"  Test RÂ²: {row['test_r2']:.4f}")
    print(f"  Test RMSE: ${row['test_rmse']:,.2f}")
    print(f"  Test MAE: ${row['test_mae']:,.2f}")
    print(f"  Test MAPE: {row['test_mape']:.2f}%")

# ì„±ëŠ¥ ë³€í™”ìœ¨ ê³„ì‚°
baseline_1d = results_df[(results_df['horizon'] == '1-day') &
                         (results_df['model'] == 'XGB_top10')].iloc[0]
for horizon in ['7-day', '30-day']:
    row = results_df[(results_df['horizon'] == horizon) &
                     (results_df['model'] == 'XGB_top10')].iloc[0]
    r2_change = ((row['test_r2'] - baseline_1d['test_r2']) / baseline_1d['test_r2']) * 100
    rmse_change = ((row['test_rmse'] - baseline_1d['test_rmse']) / baseline_1d['test_rmse']) * 100
    mape_change = ((row['test_mape'] - baseline_1d['test_mape']) / baseline_1d['test_mape']) * 100

    print(f"\n1-day â†’ {horizon} ì„±ëŠ¥ ë³€í™”:")
    print(f"  RÂ² ë³€í™”: {r2_change:+.1f}%")
    print(f"  RMSE ë³€í™”: {rmse_change:+.1f}%")
    print(f"  MAPE ë³€í™”: {mape_change:+.1f}%")

# ===== 9. ì˜ˆì¸¡ ì •í™•ë„ ë¶„í¬ =====
print("\n8. ì˜ˆì¸¡ ì •í™•ë„ ë¶„í¬ (XGBoost Top 10)")
print("-" * 70)

for horizon in ['1-day', '7-day', '30-day']:
    best_key = f'{horizon}_XGB_top10'
    if best_key in all_predictions:
        y_test, y_pred, _ = all_predictions[best_key]

        errors = y_test - y_pred
        pct_errors = (errors / y_test) * 100

        within_1pct = (np.abs(pct_errors) < 1).sum() / len(pct_errors) * 100
        within_2pct = (np.abs(pct_errors) < 2).sum() / len(pct_errors) * 100
        within_5pct = (np.abs(pct_errors) < 5).sum() / len(pct_errors) * 100
        within_10pct = (np.abs(pct_errors) < 10).sum() / len(pct_errors) * 100

        print(f"\n{horizon} ì˜ˆì¸¡ ì •í™•ë„:")
        print(f"  Â±1% ì´ë‚´: {within_1pct:.1f}%")
        print(f"  Â±2% ì´ë‚´: {within_2pct:.1f}%")
        print(f"  Â±5% ì´ë‚´: {within_5pct:.1f}%")
        print(f"  Â±10% ì´ë‚´: {within_10pct:.1f}%")

print("\n" + "=" * 70)
print("ë‹¤ì¤‘ ì˜ˆì¸¡ ê¸°ê°„ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
print("=" * 70)

# ìµœì¢… ìš”ì•½
print("\nğŸ“Š ìµœì¢… ìš”ì•½:")
print("-" * 70)
for horizon in ['1-day', '7-day', '30-day']:
    horizon_results = results_df[results_df['horizon'] == horizon].sort_values('test_r2', ascending=False)
    best = horizon_results.iloc[0]
    print(f"\n{horizon} ì˜ˆì¸¡ ìµœê³  ì„±ëŠ¥:")
    print(f"  ëª¨ë¸: {best['model']}")
    print(f"  RÂ²: {best['test_r2']:.4f} | RMSE: ${best['test_rmse']:,.2f} | MAPE: {best['test_mape']:.2f}%")
    print(f"  íŠ¹ì„±: {int(best['n_features'])}ê°œ ({best['feature_set']})")

print("\n" + "=" * 70)
