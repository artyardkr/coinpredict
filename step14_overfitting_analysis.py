import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import learning_curve
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("ê³¼ì í•© ë¶„ì„ (Overfitting Analysis)")
print("=" * 70)

# ===== 1. ë°ì´í„° ë¡œë“œ =====
print("\n1. ë°ì´í„° ë¡œë“œ")
print("-" * 70)

df = pd.read_csv('integrated_data_full.csv', index_col=0, parse_dates=True)
print(f"ì „ì²´ ë°ì´í„°: {df.shape} ({df.index[0].date()} ~ {df.index[-1].date()})")

# ===== 2. ë°ì´í„° ëˆ„ìˆ˜ ê°€ëŠ¥ì„± ìˆëŠ” íŠ¹ì„± ë¶„ì„ =====
print("\n2. ë°ì´í„° ëˆ„ìˆ˜ ìœ„í—˜ íŠ¹ì„± íƒì§€")
print("-" * 70)

# Closeì™€ì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
correlations = df.corr()['Close'].abs().sort_values(ascending=False)

print("\nCloseì™€ ìƒê´€ê³„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 20ê°œ íŠ¹ì„±:")
print("-" * 70)
for i, (col, corr) in enumerate(correlations.head(20).items(), 1):
    risk = ""
    if corr > 0.99:
        risk = "âš ï¸ ë§¤ìš° ë†’ìŒ (ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬)"
    elif corr > 0.95:
        risk = "âš ï¸ ë†’ìŒ"
    elif corr > 0.90:
        risk = "âš ï¸ ì£¼ì˜"
    print(f"{i:2d}. {col:30s} : {corr:.6f} {risk}")

# ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ íŠ¹ì„± (ìƒê´€ê³„ìˆ˜ > 0.95)
leakage_features = correlations[correlations > 0.95].index.tolist()
leakage_features = [f for f in leakage_features if f != 'Close']

print(f"\në°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ íŠ¹ì„± ({len(leakage_features)}ê°œ):")
for feat in leakage_features:
    print(f"  - {feat} (ìƒê´€ê³„ìˆ˜: {correlations[feat]:.6f})")

# ===== 3. ì œê±°í•  íŠ¹ì„± ì •ì˜ =====
print("\n3. ì œê±°í•  íŠ¹ì„± ì •ì˜")
print("-" * 70)

# ëª…ì‹œì ìœ¼ë¡œ ì œê±°í•  íŠ¹ì„±
explicit_exclude = [
    'Close',           # íƒ€ê²Ÿê³¼ ë™ì¼
    'cumulative_return',  # Closeë¡œë¶€í„° ì§ì ‘ ê³„ì‚°
    'High', 'Low', 'Open',  # ê°™ì€ ë‚ ì˜ ê°€ê²© ì •ë³´
    'bc_market_price',   # Closeì™€ ê±°ì˜ ë™ì¼
    'bc_market_cap',     # Close * Supplyë¡œ ê³„ì‚°
]

# ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ íŠ¹ì„±ë„ ì œê±°
all_exclude = list(set(explicit_exclude + leakage_features))

print(f"ì œê±°í•  íŠ¹ì„± ({len(all_exclude)}ê°œ):")
for feat in sorted(all_exclude):
    if feat in df.columns:
        print(f"  - {feat}")

# ===== 4. íƒ€ê²Ÿ ìƒì„± ë° íŠ¹ì„± ì„ íƒ =====
print("\n4. íƒ€ê²Ÿ ìƒì„± ë° íŠ¹ì„± ì„ íƒ")
print("-" * 70)

# íƒ€ê²Ÿ: ë‹¤ìŒ ë‚  ì¢…ê°€
df['target'] = df['Close'].shift(-1)
df_clean = df.dropna(subset=['target']).copy()

# íŠ¹ì„± ì„ íƒ
all_features = [col for col in df_clean.columns if col not in all_exclude and col != 'target']
X = df_clean[all_features].copy()
y = df_clean['target'].copy()

print(f"íŠ¹ì„± ìˆ˜: {len(all_features)}ê°œ")
print(f"ìƒ˜í”Œ ìˆ˜: {len(X)}ê°œ")
print(f"ê¸°ê°„: {X.index[0].date()} ~ {X.index[-1].date()}")

# ===== 5. ë°ì´í„° ë¶„í•  =====
print("\n5. ë°ì´í„° ë¶„í•  (ì‹œê³„ì—´ 80:20)")
print("-" * 70)

split_idx = int(len(X) * 0.8)

X_train = X.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_train = y.iloc[:split_idx]
y_test = y.iloc[split_idx:]

print(f"í›ˆë ¨: {X_train.shape[0]}ê°œ ({X_train.index[0].date()} ~ {X_train.index[-1].date()})")
print(f"í…ŒìŠ¤íŠ¸: {X_test.shape[0]}ê°œ ({X_test.index[0].date()} ~ {X_test.index[-1].date()})")

# ===== 6. ëª¨ë¸ í›ˆë ¨ ë° ê³¼ì í•© ë¶„ì„ =====
print("\n" + "=" * 70)
print("6. ëª¨ë¸ í›ˆë ¨ ë° ê³¼ì í•© ë¶„ì„")
print("=" * 70)

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

results = []

# Random Forest
print("\n[Random Forest]")
print("-" * 70)
rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train_scaled, y_train)

y_train_pred_rf = rf.predict(X_train_scaled)
y_test_pred_rf = rf.predict(X_test_scaled)

rf_train_r2 = r2_score(y_train, y_train_pred_rf)
rf_test_r2 = r2_score(y_test, y_test_pred_rf)
rf_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
rf_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))

print(f"Train RÂ²: {rf_train_r2:.4f} | RMSE: ${rf_train_rmse:,.2f}")
print(f"Test RÂ²:  {rf_test_r2:.4f} | RMSE: ${rf_test_rmse:,.2f}")
print(f"RÂ² ì°¨ì´:  {rf_train_r2 - rf_test_r2:.4f} {'âš ï¸ ê³¼ì í•© ì˜ì‹¬' if rf_train_r2 - rf_test_r2 > 0.1 else 'âœ“'}")

results.append({
    'model': 'Random Forest',
    'train_r2': rf_train_r2,
    'test_r2': rf_test_r2,
    'r2_gap': rf_train_r2 - rf_test_r2,
    'train_rmse': rf_train_rmse,
    'test_rmse': rf_test_rmse
})

# Feature Importance
rf_importance = pd.DataFrame({
    'feature': all_features,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
for i, row in rf_importance.head(10).iterrows():
    print(f"  {i+1}. {row['feature']:30s} : {row['importance']:.4f}")

# íŠ¹ì • íŠ¹ì„±ì´ ê³¼ë„í•˜ê²Œ ë†’ì€ì§€ í™•ì¸
top1_importance = rf_importance.iloc[0]['importance']
if top1_importance > 0.5:
    print(f"\nâš ï¸ ê²½ê³ : '{rf_importance.iloc[0]['feature']}' íŠ¹ì„±ì´ {top1_importance:.1%}ì˜ ì¤‘ìš”ë„ë¥¼ ê°€ì§ (ê³¼ì í•© ê°€ëŠ¥ì„±)")

# XGBoost
print("\n[XGBoost]")
print("-" * 70)
xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train_scaled, y_train)

y_train_pred_xgb = xgb_model.predict(X_train_scaled)
y_test_pred_xgb = xgb_model.predict(X_test_scaled)

xgb_train_r2 = r2_score(y_train, y_train_pred_xgb)
xgb_test_r2 = r2_score(y_test, y_test_pred_xgb)
xgb_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
xgb_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))

print(f"Train RÂ²: {xgb_train_r2:.4f} | RMSE: ${xgb_train_rmse:,.2f}")
print(f"Test RÂ²:  {xgb_test_r2:.4f} | RMSE: ${xgb_test_rmse:,.2f}")
print(f"RÂ² ì°¨ì´:  {xgb_train_r2 - xgb_test_r2:.4f} {'âš ï¸ ê³¼ì í•© ì˜ì‹¬' if xgb_train_r2 - xgb_test_r2 > 0.1 else 'âœ“'}")

results.append({
    'model': 'XGBoost',
    'train_r2': xgb_train_r2,
    'test_r2': xgb_test_r2,
    'r2_gap': xgb_train_r2 - xgb_test_r2,
    'train_rmse': xgb_train_rmse,
    'test_rmse': xgb_test_rmse
})

# Feature Importance
xgb_importance = pd.DataFrame({
    'feature': all_features,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
for i, row in xgb_importance.head(10).iterrows():
    print(f"  {i+1}. {row['feature']:30s} : {row['importance']:.4f}")

# íŠ¹ì • íŠ¹ì„±ì´ ê³¼ë„í•˜ê²Œ ë†’ì€ì§€ í™•ì¸
top1_importance = xgb_importance.iloc[0]['importance']
if top1_importance > 0.5:
    print(f"\nâš ï¸ ê²½ê³ : '{xgb_importance.iloc[0]['feature']}' íŠ¹ì„±ì´ {top1_importance:.1%}ì˜ ì¤‘ìš”ë„ë¥¼ ê°€ì§ (ê³¼ì í•© ê°€ëŠ¥ì„±)")

# ===== 7. ê³¼ì í•© ì§„ë‹¨ =====
print("\n" + "=" * 70)
print("7. ê³¼ì í•© ì§„ë‹¨")
print("=" * 70)

results_df = pd.DataFrame(results)

print("\nëª¨ë¸ë³„ ê³¼ì í•© ë¶„ì„:")
print("-" * 70)
for _, row in results_df.iterrows():
    print(f"\n{row['model']}:")
    print(f"  Train RÂ²: {row['train_r2']:.4f}")
    print(f"  Test RÂ²:  {row['test_r2']:.4f}")
    print(f"  RÂ² ì°¨ì´:  {row['r2_gap']:.4f}")

    # ê³¼ì í•© ì§„ë‹¨
    if row['r2_gap'] < 0.05:
        diagnosis = "âœ… ê³¼ì í•© ì—†ìŒ (ì–‘í˜¸)"
    elif row['r2_gap'] < 0.1:
        diagnosis = "âš ï¸ ê²½ë¯¸í•œ ê³¼ì í•© (í—ˆìš© ë²”ìœ„)"
    elif row['r2_gap'] < 0.2:
        diagnosis = "âš ï¸ ì¤‘ê°„ ìˆ˜ì¤€ ê³¼ì í•© (ê°œì„  í•„ìš”)"
    else:
        diagnosis = "âŒ ì‹¬ê°í•œ ê³¼ì í•© (ëª¨ë¸ ì¬ì„¤ê³„ í•„ìš”)"

    print(f"  ì§„ë‹¨:    {diagnosis}")

    # Test RÂ² ì§„ë‹¨
    if row['test_r2'] < 0:
        print(f"  âŒ Test RÂ²ê°€ ìŒìˆ˜ - ëª¨ë¸ì´ í‰ê· ë³´ë‹¤ ëª»í•¨")
    elif row['test_r2'] < 0.3:
        print(f"  âš ï¸ Test RÂ²ê°€ ë‚®ìŒ - ì˜ˆì¸¡ë ¥ ë¶€ì¡±")
    elif row['test_r2'] < 0.7:
        print(f"  âœ“ Test RÂ²ê°€ ë³´í†µ ìˆ˜ì¤€")
    else:
        print(f"  âœ… Test RÂ²ê°€ ë†’ìŒ - ìš°ìˆ˜í•œ ì˜ˆì¸¡ë ¥")

# ===== 8. ì‹œê°í™” =====
print("\n8. ê³¼ì í•© ë¶„ì„ ì‹œê°í™”")
print("-" * 70)

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 1. Train vs Test RÂ² ë¹„êµ
ax1 = axes[0, 0]
models = results_df['model']
x = np.arange(len(models))
width = 0.35

ax1.bar(x - width/2, results_df['train_r2'], width, label='Train RÂ²', alpha=0.8)
ax1.bar(x + width/2, results_df['test_r2'], width, label='Test RÂ²', alpha=0.8)
ax1.set_xlabel('Model', fontsize=11)
ax1.set_ylabel('RÂ² Score', fontsize=11)
ax1.set_title('Train vs Test RÂ² (Overfitting Check)', fontsize=12, fontweight='bold')
ax1.set_xticks(x)
ax1.set_xticklabels(models)
ax1.legend()
ax1.grid(True, alpha=0.3, axis='y')
ax1.axhline(y=0, color='r', linestyle='--', alpha=0.5)

# 2. RÂ² Gap (ê³¼ì í•© ì •ë„)
ax2 = axes[0, 1]
colors = ['orange' if gap > 0.1 else 'green' for gap in results_df['r2_gap']]
ax2.bar(models, results_df['r2_gap'], color=colors, alpha=0.7)
ax2.set_xlabel('Model', fontsize=11)
ax2.set_ylabel('RÂ² Gap (Train - Test)', fontsize=11)
ax2.set_title('Overfitting Severity (RÂ² Gap)', fontsize=12, fontweight='bold')
ax2.axhline(y=0.1, color='orange', linestyle='--', alpha=0.5, label='Threshold (0.1)')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

# 3. Feature Importance (Random Forest)
ax3 = axes[0, 2]
top_features_rf = rf_importance.head(15)
ax3.barh(range(len(top_features_rf)), top_features_rf['importance'], alpha=0.7)
ax3.set_yticks(range(len(top_features_rf)))
ax3.set_yticklabels(top_features_rf['feature'], fontsize=9)
ax3.set_xlabel('Importance', fontsize=11)
ax3.set_title('Feature Importance (Random Forest)', fontsize=12, fontweight='bold')
ax3.invert_yaxis()
ax3.grid(True, alpha=0.3, axis='x')

# 4. Feature Importance (XGBoost)
ax4 = axes[1, 0]
top_features_xgb = xgb_importance.head(15)
ax4.barh(range(len(top_features_xgb)), top_features_xgb['importance'], alpha=0.7, color='orange')
ax4.set_yticks(range(len(top_features_xgb)))
ax4.set_yticklabels(top_features_xgb['feature'], fontsize=9)
ax4.set_xlabel('Importance', fontsize=11)
ax4.set_title('Feature Importance (XGBoost)', fontsize=12, fontweight='bold')
ax4.invert_yaxis()
ax4.grid(True, alpha=0.3, axis='x')

# 5. Actual vs Predicted (Random Forest)
ax5 = axes[1, 1]
ax5.scatter(y_test, y_test_pred_rf, alpha=0.5, s=30)
ax5.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', linewidth=2, label='Perfect Prediction')
ax5.set_xlabel('Actual Price ($)', fontsize=11)
ax5.set_ylabel('Predicted Price ($)', fontsize=11)
ax5.set_title(f'Random Forest (Test RÂ²={rf_test_r2:.4f})', fontsize=12, fontweight='bold')
ax5.legend()
ax5.grid(True, alpha=0.3)

# 6. Actual vs Predicted (XGBoost)
ax6 = axes[1, 2]
ax6.scatter(y_test, y_test_pred_xgb, alpha=0.5, s=30, color='orange')
ax6.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', linewidth=2, label='Perfect Prediction')
ax6.set_xlabel('Actual Price ($)', fontsize=11)
ax6.set_ylabel('Predicted Price ($)', fontsize=11)
ax6.set_title(f'XGBoost (Test RÂ²={xgb_test_r2:.4f})', fontsize=12, fontweight='bold')
ax6.legend()
ax6.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('overfitting_analysis.png', dpi=300, bbox_inches='tight')
print("âœ“ overfitting_analysis.png")

plt.close()

# ===== 9. ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ (ìƒìœ„ íŠ¹ì„±) =====
print("\n9. ìƒìœ„ íŠ¹ì„± ê°„ ìƒê´€ê´€ê³„ ë¶„ì„")
print("-" * 70)

# RFì™€ XGBì˜ ìƒìœ„ íŠ¹ì„± í•©ì¹˜ê¸°
top_features_combined = list(set(
    rf_importance.head(10)['feature'].tolist() +
    xgb_importance.head(10)['feature'].tolist()
))

if len(top_features_combined) > 0:
    corr_matrix = X[top_features_combined].corr()

    plt.figure(figsize=(12, 10))
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                fmt='.2f', square=True, linewidths=0.5, cbar_kws={"shrink": 0.8})
    plt.title('Correlation Matrix of Top Features', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('top_features_correlation.png', dpi=300, bbox_inches='tight')
    print("âœ“ top_features_correlation.png")
    plt.close()

    # ë†’ì€ ìƒê´€ê´€ê³„ íƒì§€ (ë‹¤ì¤‘ê³µì„ ì„±)
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) > 0.8:
                high_corr_pairs.append((
                    corr_matrix.columns[i],
                    corr_matrix.columns[j],
                    corr_matrix.iloc[i, j]
                ))

    if high_corr_pairs:
        print(f"\nâš ï¸ ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ìŒ ({len(high_corr_pairs)}ê°œ, |r| > 0.8):")
        for feat1, feat2, corr in high_corr_pairs:
            print(f"  - {feat1} â†” {feat2}: {corr:.3f}")
        print("\nâ†’ ë‹¤ì¤‘ê³µì„ ì„± ê°€ëŠ¥ì„±: í•˜ë‚˜ì˜ íŠ¹ì„± ì œê±° ê³ ë ¤")
    else:
        print("\nâœ“ ë†’ì€ ìƒê´€ê´€ê³„ íŠ¹ì„± ìŒ ì—†ìŒ (ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ ì—†ìŒ)")

# ===== 10. ê²°ê³¼ ì €ì¥ =====
print("\n10. ê²°ê³¼ ì €ì¥")
print("-" * 70)

# ëª¨ë¸ ê²°ê³¼ ì €ì¥
results_df.to_csv('overfitting_analysis_results.csv', index=False)
print("âœ“ overfitting_analysis_results.csv")

# Feature Importance ì €ì¥
rf_importance.to_csv('feature_importance_rf.csv', index=False)
xgb_importance.to_csv('feature_importance_xgb.csv', index=False)
print("âœ“ feature_importance_rf.csv")
print("âœ“ feature_importance_xgb.csv")

# ë°ì´í„° ëˆ„ìˆ˜ ì˜ì‹¬ íŠ¹ì„± ì €ì¥
leakage_df = pd.DataFrame({
    'feature': correlations[correlations > 0.95].index.tolist(),
    'correlation_with_close': correlations[correlations > 0.95].values
})
leakage_df.to_csv('data_leakage_suspects.csv', index=False)
print("âœ“ data_leakage_suspects.csv")

print("\n" + "=" * 70)
print("ê³¼ì í•© ë¶„ì„ ì™„ë£Œ!")
print("=" * 70)

print("\nğŸ“Š ìµœì¢… ìš”ì•½:")
print("-" * 70)
print(f"ì œê±°ëœ íŠ¹ì„± ìˆ˜: {len(all_exclude)}ê°œ")
print(f"ì‚¬ìš©ëœ íŠ¹ì„± ìˆ˜: {len(all_features)}ê°œ")
print(f"\nëª¨ë¸ ì„±ëŠ¥:")
for _, row in results_df.iterrows():
    print(f"  {row['model']:15s}: Test RÂ² = {row['test_r2']:7.4f} (Gap: {row['r2_gap']:.4f})")

# ìµœì¢… ê¶Œì¥ì‚¬í•­
print("\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
print("-" * 70)
best_model = results_df.loc[results_df['test_r2'].idxmax()]
if best_model['r2_gap'] > 0.2:
    print("âš ï¸ ëª¨ë“  ëª¨ë¸ì—ì„œ ê³¼ì í•© ë°œê²¬ - ì •ê·œí™” ê°•í™” ë˜ëŠ” íŠ¹ì„± ì¶”ê°€ ì œê±° í•„ìš”")
elif best_model['test_r2'] < 0:
    print("âŒ ëª¨ë“  ëª¨ë¸ì˜ Test RÂ²ê°€ ìŒìˆ˜ - ë°ì´í„° ê¸°ê°„ ë˜ëŠ” íŠ¹ì„± ì¬ê²€í†  í•„ìš”")
else:
    print(f"âœ… {best_model['model']} ëª¨ë¸ ì‚¬ìš© ê¶Œì¥ (Test RÂ²: {best_model['test_r2']:.4f})")

print("=" * 70)
