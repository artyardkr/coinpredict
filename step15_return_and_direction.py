import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (mean_squared_error, mean_absolute_error, r2_score,
                             accuracy_score, precision_recall_fscore_support,
                             confusion_matrix, classification_report)
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("ë³€í™”ìœ¨ & ë°©í–¥ ì˜ˆì¸¡ ëª¨ë¸ (Return & Direction Prediction)")
print("=" * 70)

# ===== 1. ë°ì´í„° ë¡œë“œ =====
print("\n1. ë°ì´í„° ë¡œë“œ")
print("-" * 70)

df = pd.read_csv('integrated_data_full.csv', index_col=0, parse_dates=True)
print(f"ì „ì²´ ë°ì´í„°: {df.shape} ({df.index[0].date()} ~ {df.index[-1].date()})")

# 2021ë…„ ë°ì´í„°ë§Œ í•„í„°ë§ (ì„±ê³µí–ˆë˜ ì ‘ê·¼)
df_2021 = df[df.index.year == 2021].copy()
print(f"2021ë…„ ë°ì´í„°: {df_2021.shape} ({df_2021.index[0].date()} ~ {df_2021.index[-1].date()})")

# ===== 2. íƒ€ê²Ÿ ìƒì„± =====
print("\n2. íƒ€ê²Ÿ ìƒì„± (ë³€í™”ìœ¨ & ë°©í–¥)")
print("-" * 70)

# 1) ë³€í™”ìœ¨ íƒ€ê²Ÿ (Regression)
df_2021['target_return'] = df_2021['Close'].pct_change().shift(-1) * 100  # ë‚´ì¼ ìˆ˜ìµë¥ (%)

# 2) ë°©í–¥ íƒ€ê²Ÿ (Classification)
# ì„ê³„ê°’ ì„¤ì •: Â±0.5% ì´ë‚´ëŠ” íš¡ë³´ë¡œ ê°„ì£¼
threshold = 0.5

def classify_direction(return_pct):
    if pd.isna(return_pct):
        return np.nan
    elif return_pct > threshold:
        return 1  # ìƒìŠ¹
    elif return_pct < -threshold:
        return -1  # í•˜ë½
    else:
        return 0  # íš¡ë³´

df_2021['target_direction'] = df_2021['target_return'].apply(classify_direction)

# NaN ì œê±°
df_2021 = df_2021.dropna(subset=['target_return', 'target_direction'])

print(f"íƒ€ê²Ÿ ìƒì„± í›„ ìƒ˜í”Œ ìˆ˜: {len(df_2021)}")
print(f"\në³€í™”ìœ¨ í†µê³„:")
print(f"  í‰ê· : {df_2021['target_return'].mean():.2f}%")
print(f"  í‘œì¤€í¸ì°¨: {df_2021['target_return'].std():.2f}%")
print(f"  ìµœì†Œ: {df_2021['target_return'].min():.2f}%")
print(f"  ìµœëŒ€: {df_2021['target_return'].max():.2f}%")

print(f"\në°©í–¥ ë¶„í¬ (ì„ê³„ê°’: Â±{threshold}%):")
direction_counts = df_2021['target_direction'].value_counts().sort_index()
total = len(df_2021)
for direction, count in direction_counts.items():
    dir_int = int(direction)
    label = {-1: 'í•˜ë½', 0: 'íš¡ë³´', 1: 'ìƒìŠ¹'}[dir_int]
    print(f"  {label:4s} ({dir_int:2d}): {count:3d}ê°œ ({count/total*100:5.1f}%)")

# ===== 3. íŠ¹ì„± ì„ íƒ =====
print("\n3. íŠ¹ì„± ì„ íƒ")
print("-" * 70)

# ë°ì´í„° ëˆ„ìˆ˜ ì œê±°
exclude_cols = [
    'Close', 'target_return', 'target_direction',
    'cumulative_return', 'High', 'Low', 'Open',
    'bc_market_price', 'bc_market_cap'
]

all_features = [col for col in df_2021.columns if col not in exclude_cols]

# Top 10 íŠ¹ì„± ë¡œë“œ (step9ì—ì„œ ì„±ê³µí–ˆë˜ íŠ¹ì„±ë“¤)
try:
    with open('selected_features_top10.txt', 'r') as f:
        lines = f.readlines()
        top10_features = [line.strip().split('. ')[1] for line in lines if line.strip() and '. ' in line]
    print(f"âœ“ Top 10 íŠ¹ì„± ë¡œë“œ: {len(top10_features)}ê°œ")
    use_features = top10_features
except:
    print("âš ï¸ Top 10 íŠ¹ì„± íŒŒì¼ ì—†ìŒ - ëª¨ë“  íŠ¹ì„± ì‚¬ìš©")
    use_features = all_features

X = df_2021[use_features].copy()
y_return = df_2021['target_return'].copy()
y_direction = df_2021['target_direction'].copy()

print(f"íŠ¹ì„± ìˆ˜: {len(use_features)}ê°œ")
print(f"ìƒ˜í”Œ ìˆ˜: {len(X)}ê°œ")

# ===== 4. ë°ì´í„° ë¶„í•  =====
print("\n4. ë°ì´í„° ë¶„í•  (7:3)")
print("-" * 70)

split_idx = int(len(X) * 0.7)

X_train = X.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_return_train = y_return.iloc[:split_idx]
y_return_test = y_return.iloc[split_idx:]
y_direction_train = y_direction.iloc[:split_idx]
y_direction_test = y_direction.iloc[split_idx:]

print(f"í›ˆë ¨: {X_train.shape[0]}ê°œ ({X_train.index[0].date()} ~ {X_train.index[-1].date()})")
print(f"í…ŒìŠ¤íŠ¸: {X_test.shape[0]}ê°œ ({X_test.index[0].date()} ~ {X_test.index[-1].date()})")

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ===== 5. ë³€í™”ìœ¨ ì˜ˆì¸¡ (Regression) =====
print("\n" + "=" * 70)
print("5. ë³€í™”ìœ¨ ì˜ˆì¸¡ ëª¨ë¸ (Regression)")
print("=" * 70)

regression_results = []

# Random Forest Regressor
print("\n[Random Forest - Regression]")
print("-" * 70)
rf_reg = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
    n_jobs=-1
)

rf_reg.fit(X_train_scaled, y_return_train)
y_return_pred_rf = rf_reg.predict(X_test_scaled)

rf_r2 = r2_score(y_return_test, y_return_pred_rf)
rf_rmse = np.sqrt(mean_squared_error(y_return_test, y_return_pred_rf))
rf_mae = mean_absolute_error(y_return_test, y_return_pred_rf)

print(f"Test RÂ²: {rf_r2:.4f}")
print(f"Test RMSE: {rf_rmse:.4f}%")
print(f"Test MAE: {rf_mae:.4f}%")

regression_results.append({
    'model': 'Random Forest',
    'r2': rf_r2,
    'rmse': rf_rmse,
    'mae': rf_mae
})

# XGBoost Regressor
print("\n[XGBoost - Regression]")
print("-" * 70)
xgb_reg = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_reg.fit(X_train_scaled, y_return_train)
y_return_pred_xgb = xgb_reg.predict(X_test_scaled)

xgb_r2 = r2_score(y_return_test, y_return_pred_xgb)
xgb_rmse = np.sqrt(mean_squared_error(y_return_test, y_return_pred_xgb))
xgb_mae = mean_absolute_error(y_return_test, y_return_pred_xgb)

print(f"Test RÂ²: {xgb_r2:.4f}")
print(f"Test RMSE: {xgb_rmse:.4f}%")
print(f"Test MAE: {xgb_mae:.4f}%")

regression_results.append({
    'model': 'XGBoost',
    'r2': xgb_r2,
    'rmse': xgb_rmse,
    'mae': xgb_mae
})

# ===== 6. ë°©í–¥ ì˜ˆì¸¡ (Classification) =====
print("\n" + "=" * 70)
print("6. ë°©í–¥ ì˜ˆì¸¡ ëª¨ë¸ (Classification)")
print("=" * 70)

classification_results = []

# Random Forest Classifier
print("\n[Random Forest - Classification]")
print("-" * 70)
rf_clf = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
    n_jobs=-1,
    class_weight='balanced'  # ë¶ˆê· í˜• í´ë˜ìŠ¤ ì²˜ë¦¬
)

rf_clf.fit(X_train_scaled, y_direction_train)
y_direction_pred_rf = rf_clf.predict(X_test_scaled)
y_direction_proba_rf = rf_clf.predict_proba(X_test_scaled)

rf_acc = accuracy_score(y_direction_test, y_direction_pred_rf)
rf_prec, rf_rec, rf_f1, _ = precision_recall_fscore_support(
    y_direction_test, y_direction_pred_rf, average='weighted', zero_division=0
)

print(f"Accuracy: {rf_acc:.4f} ({rf_acc*100:.1f}%)")
print(f"Precision: {rf_prec:.4f}")
print(f"Recall: {rf_rec:.4f}")
print(f"F1-Score: {rf_f1:.4f}")

print(f"\ní´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
print(classification_report(y_direction_test, y_direction_pred_rf,
                          target_names=['í•˜ë½', 'íš¡ë³´', 'ìƒìŠ¹'], zero_division=0))

classification_results.append({
    'model': 'Random Forest',
    'accuracy': rf_acc,
    'precision': rf_prec,
    'recall': rf_rec,
    'f1': rf_f1
})

# XGBoost Classifier
print("\n[XGBoost - Classification]")
print("-" * 70)

# XGBoostëŠ” 0, 1, 2ë¡œ ë ˆì´ë¸” í•„ìš”
y_direction_train_xgb = y_direction_train + 1  # -1,0,1 â†’ 0,1,2
y_direction_test_xgb = y_direction_test + 1

xgb_clf = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=3,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_clf.fit(X_train_scaled, y_direction_train_xgb)
y_direction_pred_xgb_raw = xgb_clf.predict(X_test_scaled)
y_direction_pred_xgb = y_direction_pred_xgb_raw - 1  # 0,1,2 â†’ -1,0,1
y_direction_proba_xgb = xgb_clf.predict_proba(X_test_scaled)

xgb_acc = accuracy_score(y_direction_test, y_direction_pred_xgb)
xgb_prec, xgb_rec, xgb_f1, _ = precision_recall_fscore_support(
    y_direction_test, y_direction_pred_xgb, average='weighted', zero_division=0
)

print(f"Accuracy: {xgb_acc:.4f} ({xgb_acc*100:.1f}%)")
print(f"Precision: {xgb_prec:.4f}")
print(f"Recall: {xgb_rec:.4f}")
print(f"F1-Score: {xgb_f1:.4f}")

print(f"\ní´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
print(classification_report(y_direction_test, y_direction_pred_xgb,
                          target_names=['í•˜ë½', 'íš¡ë³´', 'ìƒìŠ¹'], zero_division=0))

classification_results.append({
    'model': 'XGBoost',
    'accuracy': xgb_acc,
    'precision': xgb_prec,
    'recall': xgb_rec,
    'f1': xgb_f1
})

# ===== 7. ê²°í•© ë¶„ì„ (ë³€í™”ìœ¨ + ë°©í–¥) =====
print("\n" + "=" * 70)
print("7. ê²°í•© ë¶„ì„: ë³€í™”ìœ¨ê³¼ ë°©í–¥ ë™ì‹œ í‰ê°€")
print("=" * 70)

# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
best_reg = 'XGBoost' if xgb_r2 > rf_r2 else 'Random Forest'
best_clf = 'XGBoost' if xgb_acc > rf_acc else 'Random Forest'

print(f"\nìµœê³  ì„±ëŠ¥ ëª¨ë¸:")
print(f"  ë³€í™”ìœ¨ ì˜ˆì¸¡: {best_reg} (RÂ² {max(xgb_r2, rf_r2):.4f})")
print(f"  ë°©í–¥ ì˜ˆì¸¡: {best_clf} (Accuracy {max(xgb_acc, rf_acc):.4f})")

# XGBoost ê²°ê³¼ë¡œ í†µí•© ë¶„ì„
y_return_pred = y_return_pred_xgb
y_direction_pred = y_direction_pred_xgb

# ì‹¤ì œ ê°€ê²© ë³µì›
actual_prices = []
predicted_prices = []
current_price = df_2021['Close'].iloc[split_idx - 1]  # í…ŒìŠ¤íŠ¸ ì‹œì‘ ì „ ë§ˆì§€ë§‰ ê°€ê²©

for i, (actual_return, pred_return) in enumerate(zip(y_return_test, y_return_pred)):
    actual_price = current_price * (1 + actual_return / 100)
    pred_price = current_price * (1 + pred_return / 100)

    actual_prices.append(actual_price)
    predicted_prices.append(pred_price)

    current_price = actual_price  # ì‹¤ì œ ê°€ê²©ìœ¼ë¡œ ì—…ë°ì´íŠ¸

actual_prices = np.array(actual_prices)
predicted_prices = np.array(predicted_prices)

# ê°€ê²© ì˜ˆì¸¡ ì„±ëŠ¥
price_r2 = r2_score(actual_prices, predicted_prices)
price_rmse = np.sqrt(mean_squared_error(actual_prices, predicted_prices))
price_mae = mean_absolute_error(actual_prices, predicted_prices)

print(f"\në³µì›ëœ ê°€ê²© ì˜ˆì¸¡ ì„±ëŠ¥:")
print(f"  RÂ²: {price_r2:.4f}")
print(f"  RMSE: ${price_rmse:,.2f}")
print(f"  MAE: ${price_mae:,.2f}")

# íŠ¸ë ˆì´ë”© ì‹œë®¬ë ˆì´ì…˜
print(f"\níŠ¸ë ˆì´ë”© ì‹œë®¬ë ˆì´ì…˜ (ë°©í–¥ ì˜ˆì¸¡ ê¸°ë°˜):")
print("-" * 70)

initial_capital = 10000  # $10,000 ì´ˆê¸° ìë³¸
capital = initial_capital
position = 0  # 0: í˜„ê¸ˆ, 1: ë§¤ìˆ˜
trades = []

for i, (actual_return, pred_direction) in enumerate(zip(y_return_test.values, y_direction_pred)):
    if pred_direction == 1 and position == 0:  # ìƒìŠ¹ ì˜ˆì¸¡ & í˜„ê¸ˆ ë³´ìœ  â†’ ë§¤ìˆ˜
        position = 1
        trades.append(('BUY', i, actual_return))
    elif pred_direction == -1 and position == 1:  # í•˜ë½ ì˜ˆì¸¡ & ì£¼ì‹ ë³´ìœ  â†’ ë§¤ë„
        capital *= (1 + actual_return / 100)
        position = 0
        trades.append(('SELL', i, actual_return))
    elif position == 1:  # ë³´ìœ  ì¤‘
        capital *= (1 + actual_return / 100)

# ë§ˆì§€ë§‰ì— í¬ì§€ì…˜ ì •ë¦¬
if position == 1:
    capital *= (1 + y_return_test.values[-1] / 100)

total_return = (capital - initial_capital) / initial_capital * 100
buy_hold_capital = initial_capital * (1 + y_return_test.sum() / 100)
buy_hold_return = (buy_hold_capital - initial_capital) / initial_capital * 100

print(f"ì´ˆê¸° ìë³¸: ${initial_capital:,.2f}")
print(f"ìµœì¢… ìë³¸ (ì „ëµ): ${capital:,.2f}")
print(f"ìˆ˜ìµë¥  (ì „ëµ): {total_return:+.2f}%")
print(f"\në¹„êµ: Buy & Hold")
print(f"ìµœì¢… ìë³¸: ${buy_hold_capital:,.2f}")
print(f"ìˆ˜ìµë¥ : {buy_hold_return:+.2f}%")
print(f"\nì „ëµ ìš°ìœ„: {total_return - buy_hold_return:+.2f}%p")
print(f"ê±°ë˜ íšŸìˆ˜: {len(trades)}íšŒ")

# ===== 8. ì‹œê°í™” =====
print("\n8. ì‹œê°í™”")
print("-" * 70)

fig, axes = plt.subplots(3, 3, figsize=(18, 15))

# 1. ë³€í™”ìœ¨ ì˜ˆì¸¡ (Regression)
ax1 = axes[0, 0]
ax1.scatter(y_return_test, y_return_pred_rf, alpha=0.5, s=30, label='RF')
ax1.scatter(y_return_test, y_return_pred_xgb, alpha=0.5, s=30, label='XGB')
ax1.plot([y_return_test.min(), y_return_test.max()],
         [y_return_test.min(), y_return_test.max()], 'r--', linewidth=2)
ax1.set_xlabel('Actual Return (%)', fontsize=11)
ax1.set_ylabel('Predicted Return (%)', fontsize=11)
ax1.set_title('Return Prediction (Regression)', fontsize=12, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. ë³€í™”ìœ¨ ë¶„í¬
ax2 = axes[0, 1]
ax2.hist(y_return_test, bins=30, alpha=0.5, label='Actual', color='blue')
ax2.hist(y_return_pred_xgb, bins=30, alpha=0.5, label='Predicted (XGB)', color='orange')
ax2.set_xlabel('Return (%)', fontsize=11)
ax2.set_ylabel('Frequency', fontsize=11)
ax2.set_title('Return Distribution', fontsize=12, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3, axis='y')

# 3. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Regression)
ax3 = axes[0, 2]
models_reg = ['RF', 'XGB']
r2_scores = [rf_r2, xgb_r2]
colors_reg = ['blue', 'orange']
ax3.bar(models_reg, r2_scores, color=colors_reg, alpha=0.7)
ax3.set_ylabel('RÂ² Score', fontsize=11)
ax3.set_title('Regression Model Comparison', fontsize=12, fontweight='bold')
ax3.grid(True, alpha=0.3, axis='y')
for i, score in enumerate(r2_scores):
    ax3.text(i, score + 0.01, f'{score:.3f}', ha='center', fontweight='bold')

# 4. Confusion Matrix (RF)
ax4 = axes[1, 0]
cm_rf = confusion_matrix(y_direction_test, y_direction_pred_rf, labels=[-1, 0, 1])
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=ax4,
            xticklabels=['í•˜ë½', 'íš¡ë³´', 'ìƒìŠ¹'],
            yticklabels=['í•˜ë½', 'íš¡ë³´', 'ìƒìŠ¹'])
ax4.set_xlabel('Predicted', fontsize=11)
ax4.set_ylabel('Actual', fontsize=11)
ax4.set_title('Confusion Matrix (RF)', fontsize=12, fontweight='bold')

# 5. Confusion Matrix (XGB)
ax5 = axes[1, 1]
cm_xgb = confusion_matrix(y_direction_test, y_direction_pred_xgb, labels=[-1, 0, 1])
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Oranges', ax=ax5,
            xticklabels=['í•˜ë½', 'íš¡ë³´', 'ìƒìŠ¹'],
            yticklabels=['í•˜ë½', 'íš¡ë³´', 'ìƒìŠ¹'])
ax5.set_xlabel('Predicted', fontsize=11)
ax5.set_ylabel('Actual', fontsize=11)
ax5.set_title('Confusion Matrix (XGB)', fontsize=12, fontweight='bold')

# 6. ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„ ë¹„êµ
ax6 = axes[1, 2]
models_clf = ['RF', 'XGB']
accuracies = [rf_acc, xgb_acc]
ax6.bar(models_clf, accuracies, color=['blue', 'orange'], alpha=0.7)
ax6.set_ylabel('Accuracy', fontsize=11)
ax6.set_title('Classification Accuracy', fontsize=12, fontweight='bold')
ax6.set_ylim([0, 1])
ax6.grid(True, alpha=0.3, axis='y')
for i, acc in enumerate(accuracies):
    ax6.text(i, acc + 0.02, f'{acc:.3f}', ha='center', fontweight='bold')

# 7. ë³µì›ëœ ê°€ê²© ì˜ˆì¸¡
ax7 = axes[2, 0]
test_dates = y_return_test.index
ax7.plot(test_dates, actual_prices, label='Actual', linewidth=2, color='blue')
ax7.plot(test_dates, predicted_prices, label='Predicted (XGB)',
         linewidth=2, color='red', alpha=0.7, linestyle='--')
ax7.set_xlabel('Date', fontsize=11)
ax7.set_ylabel('BTC Price ($)', fontsize=11)
ax7.set_title('Price Prediction (Reconstructed from Returns)', fontsize=12, fontweight='bold')
ax7.legend()
ax7.grid(True, alpha=0.3)
ax7.tick_params(axis='x', rotation=45)

# 8. íŠ¸ë ˆì´ë”© ì‹œë®¬ë ˆì´ì…˜
ax8 = axes[2, 1]
strategy_equity = [initial_capital]
hold_equity = [initial_capital]
current_capital = initial_capital
hold_capital = initial_capital

for actual_return, pred_dir in zip(y_return_test.values, y_direction_pred):
    if pred_dir == 1:  # ìƒìŠ¹ ì˜ˆì¸¡
        current_capital *= (1 + actual_return / 100)
    # í•˜ë½ ì˜ˆì¸¡ ì‹œ í˜„ê¸ˆ ë³´ìœ  (ë³€í™” ì—†ìŒ)
    hold_capital *= (1 + actual_return / 100)

    strategy_equity.append(current_capital)
    hold_equity.append(hold_capital)

ax8.plot(range(len(strategy_equity)), strategy_equity, label='Strategy', linewidth=2)
ax8.plot(range(len(hold_equity)), hold_equity, label='Buy & Hold', linewidth=2, linestyle='--')
ax8.set_xlabel('Trading Days', fontsize=11)
ax8.set_ylabel('Portfolio Value ($)', fontsize=11)
ax8.set_title('Trading Strategy Performance', fontsize=12, fontweight='bold')
ax8.legend()
ax8.grid(True, alpha=0.3)

# 9. Feature Importance (Classification)
ax9 = axes[2, 2]
feature_importance_clf = pd.DataFrame({
    'feature': use_features,
    'importance': xgb_clf.feature_importances_
}).sort_values('importance', ascending=False).head(10)

ax9.barh(range(len(feature_importance_clf)), feature_importance_clf['importance'], alpha=0.7)
ax9.set_yticks(range(len(feature_importance_clf)))
ax9.set_yticklabels(feature_importance_clf['feature'], fontsize=9)
ax9.set_xlabel('Importance', fontsize=11)
ax9.set_title('Feature Importance (XGB Classifier)', fontsize=12, fontweight='bold')
ax9.invert_yaxis()
ax9.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.savefig('return_and_direction_prediction.png', dpi=300, bbox_inches='tight')
print("âœ“ return_and_direction_prediction.png")

plt.close()

# ===== 9. ê²°ê³¼ ì €ì¥ =====
print("\n9. ê²°ê³¼ ì €ì¥")
print("-" * 70)

# Regression ê²°ê³¼
regression_df = pd.DataFrame(regression_results)
regression_df.to_csv('return_prediction_results.csv', index=False)
print("âœ“ return_prediction_results.csv")

# Classification ê²°ê³¼
classification_df = pd.DataFrame(classification_results)
classification_df.to_csv('direction_prediction_results.csv', index=False)
print("âœ“ direction_prediction_results.csv")

# ì˜ˆì¸¡ ê²°ê³¼
predictions_df = pd.DataFrame({
    'date': y_return_test.index,
    'actual_return': y_return_test.values,
    'predicted_return': y_return_pred,
    'actual_direction': y_direction_test.values,
    'predicted_direction': y_direction_pred,
    'actual_price': actual_prices,
    'predicted_price': predicted_prices
})
predictions_df.to_csv('predictions_return_direction.csv', index=False)
print("âœ“ predictions_return_direction.csv")

print("\n" + "=" * 70)
print("ë³€í™”ìœ¨ & ë°©í–¥ ì˜ˆì¸¡ ì™„ë£Œ!")
print("=" * 70)

print("\nğŸ“Š ìµœì¢… ìš”ì•½:")
print("-" * 70)
print(f"ë³€í™”ìœ¨ ì˜ˆì¸¡ (Regression):")
print(f"  ìµœê³  ëª¨ë¸: {best_reg}")
print(f"  RÂ²: {max(xgb_r2, rf_r2):.4f}")
print(f"  RMSE: {min(xgb_rmse, rf_rmse):.4f}%")

print(f"\në°©í–¥ ì˜ˆì¸¡ (Classification):")
print(f"  ìµœê³  ëª¨ë¸: {best_clf}")
print(f"  Accuracy: {max(xgb_acc, rf_acc):.4f} ({max(xgb_acc, rf_acc)*100:.1f}%)")
print(f"  F1-Score: {max(xgb_f1, rf_f1):.4f}")

print(f"\níŠ¸ë ˆì´ë”© ì„±ê³¼:")
print(f"  ì „ëµ ìˆ˜ìµë¥ : {total_return:+.2f}%")
print(f"  Buy & Hold: {buy_hold_return:+.2f}%")
print(f"  ì´ˆê³¼ ìˆ˜ìµ: {total_return - buy_hold_return:+.2f}%p")

print("=" * 70)
