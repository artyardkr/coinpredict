import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("Extrapolation ë¬¸ì œ í•´ê²° (ë³€í™”ìœ¨ ì˜ˆì¸¡ + Feature ì •ê·œí™”)")
print("=" * 70)

# ===== 1. ë°ì´í„° ë¡œë“œ =====
print("\n1. ë°ì´í„° ë¡œë“œ")
print("-" * 70)

df = pd.read_csv('integrated_data_full.csv', index_col=0, parse_dates=True)
print(f"ì „ì²´ ë°ì´í„°: {df.shape} ({df.index[0].date()} ~ {df.index[-1].date()})")

# ===== 2. ì œê±°í•  íŠ¹ì„± ì •ì˜ =====
print("\n2. ë°ì´í„° ëˆ„ìˆ˜ íŠ¹ì„± ì œê±°")
print("-" * 70)

exclude_features = [
    # ê°€ê²© ê´€ë ¨ (ì§ì ‘ì  ëˆ„ìˆ˜)
    'Close', 'High', 'Low', 'Open',
    'cumulative_return',
    'bc_market_price', 'bc_market_cap',

    # EMA/SMA (Closeì˜ ì´ë™í‰ê·  - ê°„ì ‘ì  ëˆ„ìˆ˜)
    'EMA5_close', 'EMA10_close', 'EMA14_close', 'EMA20_close', 'EMA30_close', 'EMA100_close',
    'SMA5_close', 'SMA10_close', 'SMA20_close', 'SMA30_close',

    # Bollinger Bands (Close ê¸°ë°˜)
    'BB_high', 'BB_mid', 'BB_low',
]

print(f"ì œê±°í•  íŠ¹ì„±: {len(exclude_features)}ê°œ")

# ===== 3. íƒ€ê²Ÿ ìƒì„± (ë³€í™”ìœ¨!) =====
print("\n3. íƒ€ê²Ÿ ìƒì„± - ì¼ë³„ ë³€í™”ìœ¨ (%) ì˜ˆì¸¡")
print("-" * 70)

# ë‹¤ìŒë‚  ìˆ˜ìµë¥  ì˜ˆì¸¡
df['target_return'] = (df['Close'].shift(-1) / df['Close'] - 1) * 100

print(f"íƒ€ê²Ÿ í†µê³„ (ì¼ë³„ ìˆ˜ìµë¥  %):")
print(f"  í‰ê· : {df['target_return'].mean():.3f}%")
print(f"  í‘œì¤€í¸ì°¨: {df['target_return'].std():.3f}%")
print(f"  ìµœì†Œ: {df['target_return'].min():.3f}%")
print(f"  ìµœëŒ€: {df['target_return'].max():.3f}%")

# ===== 4. Feature Engineering (ê°€ê²© ë…ë¦½ì ìœ¼ë¡œ) =====
print("\n4. Feature Engineering - ê°€ê²© ì˜í–¥ ì œê±°")
print("-" * 70)

# bc_miners_revenueë¥¼ ì •ê·œí™”
if 'bc_miners_revenue' in df.columns:
    df['miners_revenue_normalized'] = df['bc_miners_revenue'] / df['Close']
    print("âœ“ miners_revenue_normalized ìƒì„± (ì±„êµ´ ìˆ˜ìµ / BTC ê°€ê²©)")

# ê¸°ìˆ ì  ì§€í‘œë¥¼ ë³€í™”ìœ¨ë¡œ ë³€í™˜
for col in ['RSI', 'MACD', 'ATR', 'OBV', 'ADX', 'CCI', 'MFI']:
    if col in df.columns:
        df[f'{col}_change'] = df[col].pct_change() * 100

print("âœ“ ê¸°ìˆ ì  ì§€í‘œ ë³€í™”ìœ¨ ìƒì„±")

# ë§¤í¬ë¡œ ì§€í‘œ ë³€í™”ìœ¨
for col in ['DGS10', 'CPIAUCSL', 'UNRATE', 'M2SL']:
    if col in df.columns and df[col].notna().sum() > 0:
        df[f'{col}_change'] = df[col].pct_change() * 100

print("âœ“ ë§¤í¬ë¡œ ì§€í‘œ ë³€í™”ìœ¨ ìƒì„±")

# ===== 5. ë°ì´í„° ì •ë¦¬ =====
df_clean = df.dropna(subset=['target_return']).copy()

# íŠ¹ì„± ì„ íƒ
all_features = [col for col in df_clean.columns
                if col not in exclude_features
                and col != 'target_return'
                and not col.endswith('_change')]  # ë³€í™”ìœ¨ íŠ¹ì„±ì€ ë‚˜ì¤‘ì— ì¶”ê°€

# ë³€í™”ìœ¨ íŠ¹ì„± ì¶”ê°€
change_features = [col for col in df_clean.columns if col.endswith('_change')]
all_features.extend(change_features)

# miners_revenue_normalized ì¶”ê°€
if 'miners_revenue_normalized' in df_clean.columns:
    all_features.append('miners_revenue_normalized')
    all_features.remove('bc_miners_revenue')  # ì›ë³¸ ì œê±°

X = df_clean[all_features].copy()
y = df_clean['target_return'].copy()

# NaN ì œê±°
mask = X.notna().all(axis=1) & y.notna()
X = X[mask]
y = y[mask]

print(f"\nìµœì¢… ë°ì´í„°:")
print(f"  íŠ¹ì„± ìˆ˜: {len(all_features)}ê°œ")
print(f"  ìƒ˜í”Œ ìˆ˜: {len(X)}ê°œ")
print(f"  ê¸°ê°„: {X.index[0].date()} ~ {X.index[-1].date()}")

# ===== 6. ë°ì´í„° ë¶„í•  =====
print("\n6. ë°ì´í„° ë¶„í• ")
print("-" * 70)

# 80:20 ë¶„í• 
split_idx = int(len(X) * 0.8)

X_train = X.iloc[:split_idx]
X_test = X.iloc[split_idx:]
y_train = y.iloc[:split_idx]
y_test = y.iloc[split_idx:]

print(f"Train: {len(X_train)}ê°œ ({X_train.index[0].date()} ~ {X_train.index[-1].date()})")
print(f"Test:  {len(X_test)}ê°œ ({X_test.index[0].date()} ~ {X_test.index[-1].date()})")

# íƒ€ê²Ÿ ë¶„í¬ í™•ì¸
print(f"\nTrain íƒ€ê²Ÿ (ìˆ˜ìµë¥  %):")
print(f"  í‰ê· : {y_train.mean():.3f}%")
print(f"  í‘œì¤€í¸ì°¨: {y_train.std():.3f}%")
print(f"  ë²”ìœ„: {y_train.min():.3f}% ~ {y_train.max():.3f}%")

print(f"\nTest íƒ€ê²Ÿ (ìˆ˜ìµë¥  %):")
print(f"  í‰ê· : {y_test.mean():.3f}%")
print(f"  í‘œì¤€í¸ì°¨: {y_test.std():.3f}%")
print(f"  ë²”ìœ„: {y_test.min():.3f}% ~ {y_test.max():.3f}%")

# ===== 7. ëª¨ë¸ í›ˆë ¨ =====
print("\n" + "=" * 70)
print("7. ëª¨ë¸ í›ˆë ¨ (ë³€í™”ìœ¨ ì˜ˆì¸¡)")
print("=" * 70)

# ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Random Forest
print("\n[Random Forest]")
print("-" * 70)
rf = RandomForestRegressor(
    n_estimators=200,
    max_depth=10,
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
    n_jobs=-1
)

rf.fit(X_train_scaled, y_train)

y_train_pred_rf = rf.predict(X_train_scaled)
y_test_pred_rf = rf.predict(X_test_scaled)

rf_train_r2 = r2_score(y_train, y_train_pred_rf)
rf_test_r2 = r2_score(y_test, y_test_pred_rf)
rf_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))
rf_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))

print(f"Train RÂ²: {rf_train_r2:.4f} | RMSE: {rf_train_rmse:.3f}%")
print(f"Test RÂ²:  {rf_test_r2:.4f} | RMSE: {rf_test_rmse:.3f}%")
print(f"RÂ² ì°¨ì´:  {rf_train_r2 - rf_test_r2:.4f}")

if rf_test_r2 < 0:
    print("âš ï¸ Test RÂ² ìŒìˆ˜ - ëª¨ë¸ì´ í‰ê· ë³´ë‹¤ ëª»í•¨")
elif rf_train_r2 - rf_test_r2 > 0.1:
    print("âš ï¸ ê³¼ì í•© ì˜ì‹¬")
else:
    print("âœ… ì •ìƒ ë²”ìœ„")

# Feature Importance
rf_importance = pd.DataFrame({
    'feature': all_features,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
for idx, row in rf_importance.head(10).iterrows():
    print(f"  {row['feature']:35s} : {row['importance']:.4f}")

# XGBoost
print("\n[XGBoost]")
print("-" * 70)
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train_scaled, y_train)

y_train_pred_xgb = xgb_model.predict(X_train_scaled)
y_test_pred_xgb = xgb_model.predict(X_test_scaled)

xgb_train_r2 = r2_score(y_train, y_train_pred_xgb)
xgb_test_r2 = r2_score(y_test, y_test_pred_xgb)
xgb_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))
xgb_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))

print(f"Train RÂ²: {xgb_train_r2:.4f} | RMSE: {xgb_train_rmse:.3f}%")
print(f"Test RÂ²:  {xgb_test_r2:.4f} | RMSE: {xgb_test_rmse:.3f}%")
print(f"RÂ² ì°¨ì´:  {xgb_train_r2 - xgb_test_r2:.4f}")

if xgb_test_r2 < 0:
    print("âš ï¸ Test RÂ² ìŒìˆ˜ - ëª¨ë¸ì´ í‰ê· ë³´ë‹¤ ëª»í•¨")
elif xgb_train_r2 - xgb_test_r2 > 0.1:
    print("âš ï¸ ê³¼ì í•© ì˜ì‹¬")
else:
    print("âœ… ì •ìƒ ë²”ìœ„")

# ===== 8. ì‹¤ì œ ê°€ê²©ìœ¼ë¡œ ë³€í™˜í•´ì„œ í‰ê°€ =====
print("\n" + "=" * 70)
print("8. ì‹¤ì œ ê°€ê²© ì˜ˆì¸¡ ì„±ëŠ¥ (ë³€í™”ìœ¨ â†’ ê°€ê²© ë³€í™˜)")
print("=" * 70)

# Test ê¸°ê°„ì˜ ì‹¤ì œ Close ê°€ê²©
test_dates = y_test.index
actual_prices = df.loc[test_dates, 'Close'].values
predicted_returns_rf = y_test_pred_rf / 100  # % â†’ ë¹„ìœ¨
predicted_returns_xgb = y_test_pred_xgb / 100

# ë‹¤ìŒë‚  ì˜ˆì¸¡ ê°€ê²© = ì˜¤ëŠ˜ ê°€ê²© Ã— (1 + ì˜ˆì¸¡ ìˆ˜ìµë¥ )
predicted_prices_rf = actual_prices * (1 + predicted_returns_rf)
predicted_prices_xgb = actual_prices * (1 + predicted_returns_xgb)

# ì‹¤ì œ ë‹¤ìŒë‚  ê°€ê²©
actual_next_prices = df.loc[test_dates, 'Close'].shift(-1).values[:-1]
predicted_prices_rf = predicted_prices_rf[:-1]
predicted_prices_xgb = predicted_prices_xgb[:-1]

# ê°€ê²© ì˜ˆì¸¡ ì„±ëŠ¥
price_r2_rf = r2_score(actual_next_prices, predicted_prices_rf)
price_r2_xgb = r2_score(actual_next_prices, predicted_prices_xgb)
price_rmse_rf = np.sqrt(mean_squared_error(actual_next_prices, predicted_prices_rf))
price_rmse_xgb = np.sqrt(mean_squared_error(actual_next_prices, predicted_prices_xgb))

print("\n[Random Forest - ê°€ê²© ì˜ˆì¸¡]")
print(f"RÂ²: {price_r2_rf:.4f} | RMSE: ${price_rmse_rf:,.2f}")

print("\n[XGBoost - ê°€ê²© ì˜ˆì¸¡]")
print(f"RÂ²: {price_r2_xgb:.4f} | RMSE: ${price_rmse_xgb:,.2f}")

# ===== 9. ì‹œê°í™” =====
print("\n9. ê²°ê³¼ ì‹œê°í™”")
print("-" * 70)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. ìˆ˜ìµë¥  ì˜ˆì¸¡ (Random Forest)
ax1 = axes[0, 0]
ax1.scatter(y_test, y_test_pred_rf, alpha=0.5, s=20)
ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', linewidth=2, label='Perfect')
ax1.set_xlabel('Actual Return (%)', fontsize=11)
ax1.set_ylabel('Predicted Return (%)', fontsize=11)
ax1.set_title(f'Random Forest - Return Prediction (RÂ²={rf_test_r2:.4f})',
             fontsize=12, fontweight='bold')
ax1.legend()
ax1.grid(True, alpha=0.3)

# 2. ìˆ˜ìµë¥  ì˜ˆì¸¡ (XGBoost)
ax2 = axes[0, 1]
ax2.scatter(y_test, y_test_pred_xgb, alpha=0.5, s=20, color='orange')
ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
        'r--', linewidth=2, label='Perfect')
ax2.set_xlabel('Actual Return (%)', fontsize=11)
ax2.set_ylabel('Predicted Return (%)', fontsize=11)
ax2.set_title(f'XGBoost - Return Prediction (RÂ²={xgb_test_r2:.4f})',
             fontsize=12, fontweight='bold')
ax2.legend()
ax2.grid(True, alpha=0.3)

# 3. ê°€ê²© ì˜ˆì¸¡ (Random Forest)
ax3 = axes[1, 0]
ax3.scatter(actual_next_prices, predicted_prices_rf, alpha=0.5, s=20)
ax3.plot([actual_next_prices.min(), actual_next_prices.max()],
        [actual_next_prices.min(), actual_next_prices.max()],
        'r--', linewidth=2, label='Perfect')
ax3.set_xlabel('Actual Price ($)', fontsize=11)
ax3.set_ylabel('Predicted Price ($)', fontsize=11)
ax3.set_title(f'Random Forest - Price Prediction (RÂ²={price_r2_rf:.4f})',
             fontsize=12, fontweight='bold')
ax3.legend()
ax3.grid(True, alpha=0.3)

# 4. ê°€ê²© ì˜ˆì¸¡ (XGBoost)
ax4 = axes[1, 1]
ax4.scatter(actual_next_prices, predicted_prices_xgb, alpha=0.5, s=20, color='orange')
ax4.plot([actual_next_prices.min(), actual_next_prices.max()],
        [actual_next_prices.min(), actual_next_prices.max()],
        'r--', linewidth=2, label='Perfect')
ax4.set_xlabel('Actual Price ($)', fontsize=11)
ax4.set_ylabel('Predicted Price ($)', fontsize=11)
ax4.set_title(f'XGBoost - Price Prediction (RÂ²={price_r2_xgb:.4f})',
             fontsize=12, fontweight='bold')
ax4.legend()
ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('extrapolation_fix_results.png', dpi=300, bbox_inches='tight')
print("âœ“ extrapolation_fix_results.png")
plt.close()

# ===== 10. ê²°ê³¼ ì €ì¥ =====
print("\n10. ê²°ê³¼ ì €ì¥")
print("-" * 70)

results = pd.DataFrame({
    'model': ['Random Forest', 'XGBoost'],
    'return_train_r2': [rf_train_r2, xgb_train_r2],
    'return_test_r2': [rf_test_r2, xgb_test_r2],
    'return_r2_gap': [rf_train_r2 - rf_test_r2, xgb_train_r2 - xgb_test_r2],
    'price_test_r2': [price_r2_rf, price_r2_xgb],
    'price_rmse': [price_rmse_rf, price_rmse_xgb]
})

results.to_csv('extrapolation_fix_results.csv', index=False)
print("âœ“ extrapolation_fix_results.csv")

rf_importance.to_csv('feature_importance_return_rf.csv', index=False)
print("âœ“ feature_importance_return_rf.csv")

print("\n" + "=" * 70)
print("Extrapolation ë¬¸ì œ í•´ê²° ì™„ë£Œ!")
print("=" * 70)

print("\nğŸ“Š ìµœì¢… ìš”ì•½:")
print("-" * 70)
print(f"ë°©ë²•: ì ˆëŒ€ ê°€ê²© â†’ ì¼ë³„ ë³€í™”ìœ¨(%) ì˜ˆì¸¡")
print(f"\nìˆ˜ìµë¥  ì˜ˆì¸¡ ì„±ëŠ¥:")
for _, row in results.iterrows():
    print(f"  {row['model']:15s}: Test RÂ² = {row['return_test_r2']:7.4f} (Gap: {row['return_r2_gap']:.4f})")

print(f"\nê°€ê²© ì˜ˆì¸¡ ì„±ëŠ¥ (ë³€í™”ìœ¨â†’ê°€ê²© ë³€í™˜):")
for _, row in results.iterrows():
    print(f"  {row['model']:15s}: RÂ² = {row['price_test_r2']:7.4f} | RMSE = ${row['price_rmse']:,.2f}")

print("\nğŸ’¡ ê°œì„ ì‚¬í•­:")
print("-" * 70)
if rf_test_r2 > 0 and xgb_test_r2 > 0:
    print("âœ… Test RÂ²ê°€ ì–‘ìˆ˜ - Extrapolation ë¬¸ì œ í•´ê²°!")
else:
    print("âš ï¸ ì—¬ì „íˆ Test RÂ² ìŒìˆ˜ - ì¶”ê°€ ê°œì„  í•„ìš”")

if rf_train_r2 - rf_test_r2 < 0.2 and xgb_train_r2 - xgb_test_r2 < 0.2:
    print("âœ… ê³¼ì í•© ê°œì„ ë¨ (RÂ² Gap < 0.2)")
else:
    print("âš ï¸ ì—¬ì „íˆ ê³¼ì í•© ì¡´ì¬")

print("=" * 70)
