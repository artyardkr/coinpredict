#!/usr/bin/env python3
"""
Step 25 V2: Next-Day Price Prediction with NEW VARIABLES (138 features)

ê¸°ì¡´ step25 (88ê°œ ë³€ìˆ˜) â†’ step25_v2 (138ê°œ ë³€ìˆ˜)
ì‹ ê·œ ì¶”ê°€:
- ì¶”ê°€ ì „í†µì‹œì¥ (9ê°œ): DXY, ETH, TLT, GLD ë“±
- Fed ìœ ë™ì„± (8ê°œ): WALCL, RRPONTSYD, FED_NET_LIQUIDITY ë“±
- ê³ ê¸‰ ì˜¨ì²´ì¸ (21ê°œ): NVT, Puell Multiple, Hash Ribbon ë“±
- Bitcoin ETF (12ê°œ): IBIT, FBTC, GBTC Premium ë“±
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

# ========================================
# 1. Load Data V2 (138 features)
# ========================================
print("=" * 80)
print("Next-Day Price Prediction V2 (NEW VARIABLES)")
print("=" * 80)

df = pd.read_csv('integrated_data_full_v2.csv')
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values('Date').reset_index(drop=True)

print(f"Data shape: {df.shape}")
print(f"Date range: {df['Date'].min()} to {df['Date'].max()}")
print(f"â­ V2 Features: {df.shape[1]} (ê¸°ì¡´ 88ê°œ â†’ ì‹ ê·œ 138ê°œ, +50ê°œ)")

# ========================================
# 2. Create Target: NEXT DAY Close
# ========================================
print("\n" + "=" * 80)
print("Creating target: NEXT DAY Close")
print("=" * 80)

df['target'] = df['Close'].shift(-1)
df = df[:-1].copy()

print(f"Target created: {len(df)} samples")
print(f"Example:")
print(f"  Date: {df['Date'].iloc[0]} â†’ Features from this day")
print(f"  Close: ${df['Close'].iloc[0]:.2f} (today)")
print(f"  Target: ${df['target'].iloc[0]:.2f} (tomorrow)")

# ========================================
# 3. Feature Preparation
# ========================================
print("\n" + "=" * 80)
print("Preparing features...")
print("=" * 80)

exclude_cols = [
    'Date', 'Close', 'High', 'Low', 'Open', 'target',
    'cumulative_return',
    'bc_market_price', 'bc_market_cap',
]

ema_sma_cols = [col for col in df.columns if ('EMA' in col or 'SMA' in col) and 'close' in col.lower()]
exclude_cols.extend(ema_sma_cols)
bb_cols = [col for col in df.columns if col.startswith('BB_')]
exclude_cols.extend(bb_cols)
exclude_cols = list(set(exclude_cols))

feature_cols = [col for col in df.columns if col not in exclude_cols]
print(f"Total features: {len(feature_cols)}")

# ì‹ ê·œ ë³€ìˆ˜ í™•ì¸
new_vars_keywords = ['DXY', 'ETH', 'TLT', 'GLD', 'WALCL', 'RRPONTSYD', 'FED_NET_LIQUIDITY',
                     'NVT', 'Puell', 'Hash_Ribbon', 'IBIT', 'FBTC', 'GBTC_Premium']
new_vars_found = [col for col in feature_cols if any(kw in col for kw in new_vars_keywords)]
print(f"ì‹ ê·œ ë³€ìˆ˜ í™•ì¸: {len(new_vars_found)}ê°œ")
print(f"  ì˜ˆì‹œ: {new_vars_found[:10]}")

for col in feature_cols:
    df[col] = df[col].replace([np.inf, -np.inf], np.nan)
    df[col] = df[col].fillna(method='ffill').fillna(method='bfill')

# ========================================
# 4. Train/Test Split
# ========================================
print("\n" + "=" * 80)
print("Train/Test split...")
print("=" * 80)

split_idx = int(len(df) * 0.7)
split_date = df['Date'].iloc[split_idx]

train_mask = df['Date'] < split_date
test_mask = df['Date'] >= split_date

X_train = df[train_mask][feature_cols].values
X_test = df[test_mask][feature_cols].values
y_train = df[train_mask]['target'].values
y_test = df[test_mask]['target'].values

dates_train = df[train_mask]['Date'].values
dates_test = df[test_mask]['Date'].values
close_train = df[train_mask]['Close'].values
close_test = df[test_mask]['Close'].values

print(f"Split date: {split_date}")
print(f"Train: {len(X_train)} samples, {X_train.shape[1]} features")
print(f"Test: {len(X_test)} samples")

# Feature scaling
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

# ========================================
# 5. Define Models
# ========================================
print("\n" + "=" * 80)
print("Testing models with V2 features...")
print("=" * 80)

models = {
    'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000),
    'Ridge': Ridge(alpha=1.0),
    'Lasso': Lasso(alpha=1.0, max_iter=10000),
    'Linear Regression': LinearRegression(),
    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=10,
                                          min_samples_split=20, min_samples_leaf=10,
                                          random_state=42, n_jobs=-1),
    'XGBoost': xgb.XGBRegressor(n_estimators=200, max_depth=7, learning_rate=0.05,
                                subsample=0.8, colsample_bytree=0.8,
                                random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(n_estimators=200, max_depth=5,
                                                   learning_rate=0.05, subsample=0.8,
                                                   random_state=42),
}

if LIGHTGBM_AVAILABLE:
    models['LightGBM'] = lgb.LGBMRegressor(n_estimators=200, max_depth=7, learning_rate=0.05,
                                          subsample=0.8, colsample_bytree=0.8,
                                          random_state=42, n_jobs=-1, verbose=-1)

results = []
predictions_dict = {}

for model_name, model in models.items():
    print(f"\n{'='*60}")
    print(f"{model_name}")
    print(f"{'='*60}")

    # Train
    model.fit(X_train_scaled, y_train)

    # Predict
    y_pred_train = model.predict(X_train_scaled)
    y_pred_test = model.predict(X_test_scaled)

    # Metrics
    r2_train = r2_score(y_train, y_pred_train)
    r2_test = r2_score(y_test, y_pred_test)
    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
    mae_test = mean_absolute_error(y_test, y_pred_test)

    # Direction accuracy
    actual_direction = (y_test > close_test).astype(int)
    pred_direction = (y_pred_test > close_test).astype(int)
    direction_acc = (actual_direction == pred_direction).mean()

    print(f"Train RÂ²: {r2_train:.4f}, RMSE: ${rmse_train:.2f}")
    print(f"Test RÂ²: {r2_test:.4f}, RMSE: ${rmse_test:.2f}, MAE: ${mae_test:.2f}")
    print(f"Direction Accuracy: {direction_acc:.2%}")

    results.append({
        'Model': model_name,
        'Train RÂ²': r2_train,
        'Test RÂ²': r2_test,
        'Train RMSE': rmse_train,
        'Test RMSE': rmse_test,
        'Test MAE': mae_test,
        'Direction Acc': direction_acc,
    })

    predictions_dict[model_name] = y_pred_test

results_df = pd.DataFrame(results)

# ========================================
# 6. V1 vs V2 Comparison
# ========================================
print("\n" + "=" * 80)
print("V1 (88 features) vs V2 (138 features) ë¹„êµ")
print("=" * 80)

# step25 ì›ë³¸ ê²°ê³¼ (ì°¸ê³ ìš© - ì‹¤ì œ ê°’ì€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
v1_elasticnet_r2 = 0.8198  # ì˜ˆìƒì¹˜

v2_elasticnet_r2 = results_df[results_df['Model']=='ElasticNet']['Test RÂ²'].values[0]

print(f"""
V1 (ê¸°ì¡´ 88ê°œ ë³€ìˆ˜):
  ElasticNet Test RÂ²: {v1_elasticnet_r2:.4f}

V2 (ì‹ ê·œ 138ê°œ ë³€ìˆ˜):
  ElasticNet Test RÂ²: {v2_elasticnet_r2:.4f}

ì°¨ì´: {v2_elasticnet_r2 - v1_elasticnet_r2:+.4f}
{'âœ… ì„±ëŠ¥ í–¥ìƒ!' if v2_elasticnet_r2 > v1_elasticnet_r2 else 'âš ï¸ ì„±ëŠ¥ ìœ ì§€ ë˜ëŠ” ê°ì†Œ'}

ì‹ ê·œ ì¶”ê°€ ë³€ìˆ˜ (+50ê°œ):
  - ì¶”ê°€ ì „í†µì‹œì¥ (9ê°œ): DXY, ETH, TLT, GLD ë“±
  - Fed ìœ ë™ì„± (8ê°œ): WALCL, RRPONTSYD, FED_NET_LIQUIDITY
  - ê³ ê¸‰ ì˜¨ì²´ì¸ (21ê°œ): NVT, Puell Multiple, Hash Ribbon
  - Bitcoin ETF (12ê°œ): IBIT, FBTC, GBTC Premium
""")

# ========================================
# 7. Results Summary
# ========================================
print("\n" + "=" * 80)
print("RESULTS SUMMARY (V2)")
print("=" * 80)

results_sorted = results_df.sort_values('Test RÂ²', ascending=False)
print("\n" + results_sorted.to_string(index=False))

best_model = results_sorted.iloc[0]
print(f"\nğŸ† Best Model: {best_model['Model']}")
print(f"   Test RÂ²: {best_model['Test RÂ²']:.4f}")
print(f"   Test RMSE: ${best_model['Test RMSE']:.2f}")
print(f"   Direction Accuracy: {best_model['Direction Acc']:.2%}")

# ========================================
# 8. Visualization
# ========================================
print("\n" + "=" * 80)
print("Creating visualizations...")
print("=" * 80)

fig = plt.figure(figsize=(20, 14))
gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)

# 1. RÂ² Comparison
ax1 = fig.add_subplot(gs[0, 0])
colors = ['#2ecc71' if 'ElasticNet' in x or 'Ridge' in x or 'Lasso' in x or 'Linear' in x
          else '#3498db' for x in results_sorted['Model']]
bars = ax1.barh(range(len(results_sorted)), results_sorted['Test RÂ²'], color=colors, alpha=0.7)
ax1.set_yticks(range(len(results_sorted)))
ax1.set_yticklabels(results_sorted['Model'], fontsize=9)
ax1.set_xlabel('Test RÂ²', fontweight='bold')
ax1.set_title('Test RÂ² Comparison (V2 - 138 features)', fontweight='bold')
ax1.axvline(x=0, color='red', linestyle='--', alpha=0.3)
ax1.invert_yaxis()
ax1.grid(True, alpha=0.3, axis='x')

for i, (idx, row) in enumerate(results_sorted.iterrows()):
    ax1.text(row['Test RÂ²'], i, f"  {row['Test RÂ²']:.3f}",
            va='center', fontsize=9, fontweight='bold')

# 2. RMSE Comparison
ax2 = fig.add_subplot(gs[0, 1])
bars = ax2.barh(range(len(results_sorted)), results_sorted['Test RMSE'], color=colors, alpha=0.7)
ax2.set_yticks(range(len(results_sorted)))
ax2.set_yticklabels(results_sorted['Model'], fontsize=9)
ax2.set_xlabel('Test RMSE ($)', fontweight='bold')
ax2.set_title('Test RMSE Comparison', fontweight='bold')
ax2.invert_yaxis()
ax2.grid(True, alpha=0.3, axis='x')

# 3. Direction Accuracy
ax3 = fig.add_subplot(gs[0, 2])
bars = ax3.barh(range(len(results_sorted)), results_sorted['Direction Acc']*100, color=colors, alpha=0.7)
ax3.set_yticks(range(len(results_sorted)))
ax3.set_yticklabels(results_sorted['Model'], fontsize=9)
ax3.set_xlabel('Direction Accuracy (%)', fontweight='bold')
ax3.set_title('ìƒìŠ¹/í•˜ë½ ë°©í–¥ ì •í™•ë„', fontweight='bold')
ax3.axvline(x=50, color='red', linestyle='--', alpha=0.3, label='Random')
ax3.invert_yaxis()
ax3.grid(True, alpha=0.3, axis='x')
ax3.legend()

# 4. V1 vs V2 comparison
ax4 = fig.add_subplot(gs[1, 0])
comparison_data = {
    'V1\n(88ê°œ ë³€ìˆ˜)': v1_elasticnet_r2,
    'V2\n(138ê°œ ë³€ìˆ˜)\n+50ê°œ ì¶”ê°€': v2_elasticnet_r2
}
bars = ax4.bar(comparison_data.keys(), comparison_data.values(),
               color=['#3498db', '#2ecc71'], alpha=0.7)
ax4.set_ylabel('ElasticNet Test RÂ²', fontweight='bold')
ax4.set_title('V1 vs V2: ElasticNet ì„±ëŠ¥ ë¹„êµ', fontweight='bold', fontsize=12)
ax4.axhline(y=0, color='black', linestyle='--', alpha=0.3)
ax4.grid(True, alpha=0.3, axis='y')

for bar in bars:
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold')

# 5-7. Time series predictions (Top 3 models)
top_3 = results_sorted.head(3)

for idx, (_, row) in enumerate(top_3.iterrows()):
    ax = fig.add_subplot(gs[1+idx//2, 1+idx%2])

    model_name = row['Model']
    predictions = predictions_dict[model_name]

    ax.plot(dates_test, y_test, label='ì‹¤ì œ (ë‚´ì¼)',
            linewidth=2, color='black', alpha=0.8)
    ax.plot(dates_test, predictions, label=f'ì˜ˆì¸¡ (ë‚´ì¼)',
            linewidth=2, color='#e74c3c', alpha=0.7, linestyle='--')
    ax.plot(dates_test, close_test, label='ì˜¤ëŠ˜',
            linewidth=1, color='gray', alpha=0.5, linestyle=':')

    ax.set_xlabel('ë‚ ì§œ', fontweight='bold', fontsize=10)
    ax.set_ylabel('ê°€ê²© ($)', fontweight='bold', fontsize=10)
    title = f"#{idx+1}: {model_name}\nRÂ²={row['Test RÂ²']:.3f}, ë°©í–¥={row['Direction Acc']:.1%}"
    ax.set_title(title, fontweight='bold', fontsize=10)
    ax.legend(fontsize=8)
    ax.grid(True, alpha=0.3)
    ax.tick_params(axis='x', rotation=45)

# 8. Prediction Error Distribution (Best Model)
ax8 = fig.add_subplot(gs[2, 2])
best_model_name = results_sorted.iloc[0]['Model']
best_predictions = predictions_dict[best_model_name]
errors = y_test - best_predictions
ax8.hist(errors, bins=50, color='#3498db', alpha=0.7, edgecolor='black')
ax8.axvline(x=0, color='red', linestyle='--', linewidth=2, label='ì™„ë²½')
ax8.set_xlabel('ì˜ˆì¸¡ ì˜¤ì°¨ ($)', fontweight='bold')
ax8.set_ylabel('ë¹ˆë„', fontweight='bold')
ax8.set_title(f'ì˜¤ì°¨ ë¶„í¬: {best_model_name}', fontweight='bold')
ax8.legend()
ax8.grid(True, alpha=0.3)

mean_error = errors.mean()
std_error = errors.std()
ax8.text(0.05, 0.95, f'í‰ê· : ${mean_error:.2f}\ní‘œì¤€í¸ì°¨: ${std_error:.2f}',
        transform=ax8.transAxes, va='top', fontsize=9,
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

# 9. Actual vs Predicted Scatter (Best Model)
ax9 = fig.add_subplot(gs[3, :2])
ax9.scatter(y_test, best_predictions, alpha=0.5, s=20, color='#3498db')
ax9.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],
         'r--', linewidth=2, label='ì™„ë²½í•œ ì˜ˆì¸¡')
ax9.set_xlabel('ì‹¤ì œ ë‚´ì¼ ê°€ê²© ($)', fontweight='bold', fontsize=11)
ax9.set_ylabel('ì˜ˆì¸¡ ë‚´ì¼ ê°€ê²© ($)', fontweight='bold', fontsize=11)
ax9.set_title(f'ì‹¤ì œ vs ì˜ˆì¸¡: {best_model_name}', fontweight='bold', fontsize=12)
ax9.legend()
ax9.grid(True, alpha=0.3)

ax9.text(0.05, 0.95, f"RÂ² = {results_sorted.iloc[0]['Test RÂ²']:.4f}\nRMSE = ${results_sorted.iloc[0]['Test RMSE']:.2f}",
        transform=ax9.transAxes, va='top', fontsize=11, fontweight='bold',
        bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

# 10. Feature count comparison
ax10 = fig.add_subplot(gs[3, 2])
feature_categories = {
    'V1 ê¸°ì¡´': 88,
    'V2 ì‹ ê·œ': 138
}
bars = ax10.bar(feature_categories.keys(), feature_categories.values(),
               color=['#3498db', '#2ecc71'], alpha=0.7)
ax10.set_ylabel('ë³€ìˆ˜ ê°œìˆ˜', fontweight='bold')
ax10.set_title('V1 vs V2 ë³€ìˆ˜ ê°œìˆ˜ ë¹„êµ', fontweight='bold')
ax10.grid(True, alpha=0.3, axis='y')

for bar in bars:
    height = bar.get_height()
    ax10.text(bar.get_x() + bar.get_width()/2., height,
            f'{int(height)}ê°œ', ha='center', va='bottom', fontsize=11, fontweight='bold')

plt.savefig('next_day_price_prediction_v2.png', dpi=300, bbox_inches='tight')
print("Saved: next_day_price_prediction_v2.png")

# ========================================
# 9. Save Results
# ========================================
results_df.to_csv('next_day_price_prediction_v2_results.csv', index=False)
print("Saved: next_day_price_prediction_v2_results.csv")

# ========================================
# 10. Summary & Insights
# ========================================
print("\n" + "=" * 80)
print("SUMMARY & KEY INSIGHTS (V2)")
print("=" * 80)

best = results_sorted.iloc[0]

print(f"""
ğŸ“Š V2 ë‹¤ìŒë‚  ê°€ê²© ì˜ˆì¸¡ ê²°ê³¼ (138ê°œ ë³€ìˆ˜)

1. ìµœê³  ì„±ëŠ¥:
   ğŸ† {best['Model']}
   - Test RÂ²: {best['Test RÂ²']:.4f}
   - Test RMSE: ${best['Test RMSE']:.2f}
   - Direction Accuracy: {best['Direction Acc']:.2%}

2. V1 vs V2 ë¹„êµ:
   V1 (88ê°œ ë³€ìˆ˜) ElasticNet RÂ²: {v1_elasticnet_r2:.4f}
   V2 (138ê°œ ë³€ìˆ˜) ElasticNet RÂ²: {v2_elasticnet_r2:.4f}

   ì°¨ì´: {v2_elasticnet_r2 - v1_elasticnet_r2:+.4f} ({(v2_elasticnet_r2 - v1_elasticnet_r2)/v1_elasticnet_r2*100:+.1f}%)
   {'âœ… ì„±ëŠ¥ í–¥ìƒ!' if v2_elasticnet_r2 > v1_elasticnet_r2 else 'âš ï¸ ì„±ëŠ¥ ìœ ì§€'}

3. ì‹ ê·œ ë³€ìˆ˜ íš¨ê³¼:
   ì¶”ê°€ëœ 50ê°œ ë³€ìˆ˜:
   - ì¶”ê°€ ì „í†µì‹œì¥ (9ê°œ): DXY, ETH, TLT, GLD, DIA, IWM, HYG, LQD, VIX
   - Fed ìœ ë™ì„± (8ê°œ): WALCL, RRPONTSYD, FED_NET_LIQUIDITY, T10Y3M, SOFR
   - ê³ ê¸‰ ì˜¨ì²´ì¸ (21ê°œ): NVT, Puell Multiple, Hash Ribbon, Difficulty Ribbon
   - Bitcoin ETF (12ê°œ): IBIT, FBTC, GBTC Premium, Total ETF Volume

4. ë°©í–¥ ì˜ˆì¸¡ ì •í™•ë„:
   ìµœê³ : {results_sorted['Direction Acc'].max():.2%} ({results_sorted.loc[results_sorted['Direction Acc'].idxmax(), 'Model']})
   vs Random: 50%
   {'âœ… ìœ ì˜ë¯¸í•œ ì˜ˆì¸¡ë ¥!' if results_sorted['Direction Acc'].max() > 0.55 else 'âš ï¸ ê°œì„  í•„ìš”'}

5. ê²°ë¡ :
   {'âœ… V2 ë³€ìˆ˜ ì¶”ê°€ë¡œ ì„±ëŠ¥ í–¥ìƒ' if v2_elasticnet_r2 > v1_elasticnet_r2 else 'âš ï¸ V2 ë³€ìˆ˜ê°€ ì•„ì§ íš¨ê³¼ì ì´ì§€ ì•ŠìŒ'}
   {'âœ… ì‹¤ì „ í™œìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€' if best['Test RÂ²'] > 0.1 and best['Direction Acc'] > 0.55 else 'âš ï¸ ì¶”ê°€ ê°œì„  í•„ìš”'}

   ì¶”ì²œ ëª¨ë¸: {best['Model']}
   (RÂ²={best['Test RÂ²']:.3f}, Direction={best['Direction Acc']:.1%})
""")

print("\n" + "=" * 80)
print("Step 25 V2 Completed!")
print("=" * 80)
