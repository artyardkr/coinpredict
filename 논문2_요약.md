# 논문 2 요약: Helformer

## 논문 기본 정보

**제목**: Helformer: an attention-based deep learning model for cryptocurrency price forecasting

**저자**:
- T. O. Kehinde (The Hong Kong Polytechnic University) - 주저자
- Oluyinka J. Adedokun (University of Alabama in Huntsville)
- Akpan Joseph (Durban University of Technology)
- Kareem Morenikeji Kabirat (Federal University of Agriculture, Nigeria)
- Hammed Adebayo Akano (Deakin University, Australia)
- Oludolapo A. Olanrewaju (Durban University of Technology)

**출처**: Journal of Big Data (2025) 12:81

**DOI**: https://doi.org/10.1186/s40537-025-01135-4

**데이터 기간**: 2017년 1월 1일 ~ 2024년 6월 30일

**플랫폼**: Google Colab (NVIDIA A100 GPU)

---

## 연구 배경

### 암호화폐 시장의 현황 (2025년 3월 기준)

- **10,700개 이상**의 활성 암호화폐
- **4.2억 명** 이상의 사용자
- 총 시가총액: **2.54조 달러**
- 251개의 현물 거래소
- 상위 20개 코인이 전체 시장의 **90%** 차지

### BTC의 성장
- 2009년 출시: 0원
- 2024년 12월 5일: **$103,900.47** (사상 최고가)
- 가장 가치있고 인기있는 암호화폐

### 예측의 어려움

**시장 특성**:
- 24/7 거래 (공식 개장/폐장 없음)
- 높은 가격 변동성
- 비선형성 (Non-linear)
- 비정상성 (Non-stationary)
- 계절성 (Seasonality)

**봇 거래 지배**:
- 거래량의 **50% 이상**이 봇
- 강력한 딥러닝 모델 필요

---

## 기존 연구의 한계

### 1. 통계적 방법 (Classical)
- ARIMA, SARIMA, GARCH
- ❌ 선형성 가정 (암호화폐는 비선형)
- ❌ 정규분포 가정 (비현실적)

### 2. 머신러닝
- SVM, KNN, Random Forest, XGBoost
- ❌ 과적합 취약 (LSTF)
- ❌ 큰 예측 오차
- ❌ 트레이딩 전략 적용 시 성능 저하

### 3. 딥러닝
- RNN, LSTM, BiLSTM, GRU
- ✅ 복잡한 패턴 포착
- ❌ Vanishing/Exploding Gradient
- ❌ 순차 처리로 속도 느림
- ❌ 계절성/비정상성 처리 어려움

### 4. Transformer
- ✅ 병렬 처리 (빠름)
- ✅ Self-attention
- ✅ 장거리 의존성 포착
- ❌ 시계열 예측에 직접 적용 어려움
- ❌ 계절성/비정상성 미처리
- ❌ 높은 메모리/시간 복잡도

---

## Helformer: 제안 모델

### 핵심 아이디어

> **Holt-Winters 지수 평활법 + Transformer = Helformer**

**혁신**:
1. 시계열 분해로 비정상성/계절성 자동 처리
2. Encoder만 사용 (복잡도 감소)
3. FFN 대신 LSTM 사용 (시간 의존성)
4. Bayesian Optimization으로 하이퍼파라미터 튜닝

### 모델 아키텍처

```
입력 (30일 종가)
    ↓
┌─────────────────────────┐
│ Holt-Winters 분해       │
│ - Level (Lt)            │
│ - Seasonality (St)      │
│ - Deseasonalized (Yt)   │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│ Multi-Head Attention    │
│ (4 heads, size 16)      │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│ Add & Norm              │
│ (Residual + LayerNorm)  │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│ LSTM Layer              │
│ (30 neurons)            │
└─────────────────────────┘
    ↓
┌─────────────────────────┐
│ Dense Layer             │
└─────────────────────────┘
    ↓
예측 (다음날 종가)
```

### Holt-Winters 분해

**Level (수준)**:
```
Lt = α * (Xt / St) + (1 - α) * Lt-1
```
- α (alpha): 수준 평활화 계수 (0~1)
- 현재 관측값과 이전 수준의 가중 평균

**Seasonality (계절성)**:
```
St+m = γ * (Xt / Lt) + (1 - γ) * St
```
- γ (gamma): 계절 평활화 계수 (0~1)
- 365일 주기 패턴

**Deseasonalized (계절성 제거)**:
```
Yt = Xt / (St * Lt)
```
- 계절성이 제거된 데이터
- Attention 메커니즘의 입력

### Multi-Head Attention

**Self-Attention**:
```
Attention(Q, K, V) = softmax(QK^T / √d) * V
```

**Multi-Head**:
```
MultiHead(Q, K, V) = Concat(head1, ..., headh) * W^O
head_i = Attention(Q*W^Q_i, K*W^K_i, V*W^V_i)
```

**특징**:
- 4개 헤드
- 각 헤드 크기: 16
- 병렬 처리로 전역 의존성 포착

### LSTM 레이어

**역할**:
- 전통 Transformer의 FFN 대체
- 시간 의존성 포착
- 시계열 예측에 특화

**설정**:
- 30개 뉴런
- Dropout: 0.1

---

## 실험 설정

### 데이터

**BTC 메인 학습 데이터**:
- 샘플 수: 2,738개
- 기간: 2017.01.01 ~ 2024.06.30
- 최소가: $777.75
- 최고가: $73,083.50
- 평균: $21,908.94
- 표준편차: $18,749.32

**전이학습 검증 (15개 코인)**:
| 코인 | 샘플 수 | 시작일 | 평균가 | 표준편차 |
|------|--------|--------|--------|---------|
| ETH | 2,426 | 2017.11 | $1,381.28 | $1,195.18 |
| BNB | 2,426 | 2017.11 | $190.99 | $191.57 |
| SOL | 1,543 | 2020.04 | $56.29 | $60.04 |
| XRP | 2,426 | 2017.11 | $0.52 | $0.32 |
| DOGE | 2,426 | 2017.11 | $0.06 | $0.08 |
| ADA | 2,426 | 2017.11 | $0.47 | $0.55 |
| TRX | 2,426 | 2017.11 | $0.05 | $0.03 |
| AVAX | 1,380 | 2020.07 | $31.50 | $26.63 |
| SHIB | 1,171 | 2021.04 | $0.00002 | $0.00001 |
| DOT | 1,411 | 2020.08 | $13.35 | $11.49 |
| LINK | 2,426 | 2017.11 | $9.46 | $9.44 |
| BCH | 2,426 | 2017.11 | $427.86 | $409.18 |
| TON | 1,039 | 2021.08 | $2.35 | $1.50 |
| LEO | 1,868 | 2019.05 | $3.06 | $1.64 |
| NEAR | 1,356 | 2020.10 | $4.64 | $3.82 |

**데이터 출처**: Yahoo Finance

### 전처리

1. **결측값**: 없음 (고품질)
2. **이상치**: 유지 (시장 변동성 반영)
3. **정규화**: MinMaxScaler (0~1)
   ```
   y_t = (y_t - min(y_t)) / (max(y_t) - min(y_t))
   ```
4. **계절성 확인**:
   - 365일 주기 패턴 존재
   - ACF 테스트로 비정상성 확인
   - 50+ lag까지 높은 자기상관

### 하이퍼파라미터

| 파라미터 | 값 | 설명 |
|---------|-----|------|
| `time_window` | 30 | 과거 30일로 다음날 예측 |
| `num_transformer_blocks` | 1 | Transformer 블록 수 |
| `num_heads` | 4 | Attention 헤드 수 |
| `head_size` | 16 | 각 헤드 차원 |
| `dropout` | 0.1 | 드롭아웃 비율 |
| `neurons` | 30 | LSTM 뉴런 수 |
| `epochs` | 100 | 학습 에포크 |
| `batch_size` | 32 | 배치 크기 |
| `learning_rate` | 0.001 | 학습률 |
| `optimizer` | Adam | 최적화 알고리즘 |
| `loss` | MSE | 손실 함수 |
| `activation` | Mish | 활성화 함수 |
| `train_test_split` | 80:20 | 학습/테스트 분할 |
| `validation_split` | 0.2 | 검증 분할 |

### Mish 활성화 함수

```
f(x) = x * tanh(ln(1 + e^x))
```

**ReLU/Swish 대비 장점**:
- ✅ Dying ReLU 문제 해결
- ✅ 음수 값 전파 (더 많은 패턴 포착)
- ✅ 연속적 미분 (안정적 그래디언트)
- ✅ Self-gating 특성

**실험 증거**:
- 여러 연구에서 ReLU/Swish보다 낮은 예측 오차

### Bayesian 최적화 (Optuna)

**Grid/Random Search 대비 장점**:
- ✅ 적은 평가 횟수로 최적해 찾기
- ✅ 고차원 공간에서 효율적
- ✅ 확률 모델 기반 (Gaussian Process/TPE)
- ✅ Pruner 콜백으로 조기 종료

**Acquisition Function**:
- Probability of Improvement (PI)
- Expected Improvement (EI) - 주로 사용
- 탐색(Exploration)과 활용(Exploitation) 균형

**최적화 대상**:
- Learning rate
- Dropout rate
- Number of heads
- Head size
- Batch size

---

## 벤치마크 모델

Helformer와 비교한 모델들:

1. **RNN** (Recurrent Neural Network)
2. **LSTM** (Long Short-Term Memory)
3. **BiLSTM** (Bidirectional LSTM)
4. **GRU** (Gated Recurrent Unit)
5. **Transformer** (Original)

**공통 설정**:
- Epochs: 100
- Batch size: 32
- Optimizer: Adam
- Loss: MSE
- Activation: Mish
- Train/Test: 80:20

---

## 주요 결과

### 1. BTC 예측 성능

**Helformer가 모든 벤치마크 모델을 능가**:
- ✅ 가장 낮은 MSE
- ✅ 가장 낮은 MAE
- ✅ 가장 높은 R²
- ✅ 가장 낮은 RMSE

**구체적 수치는 논문 참조** (표와 그래프 제공)

### 2. Transfer Learning 검증

**BTC 모델을 15개 코인에 적용**:
- ✅ 모든 코인에서 우수한 성능
- ✅ 일반화 능력 입증
- ✅ 새로운 코인 예측 가능

**의의**:
- 코인마다 개별 모델 불필요
- 10,700개 코인 시대에 실용적
- 새로 출시되는 코인도 즉시 예측

### 3. 기존 연구와 비교

**논문에서 비교한 최신 연구들**:
- 다양한 LSTM 변형
- Transformer 변형 (TFT, Informer 등)
- 하이브리드 모델 (GARCH+ANN, MFDFA+LSTM 등)

**결과**: Helformer가 **모든 기존 연구 능가**

### 4. 트레이딩 전략 검증

**단순 전략 구현**:
- 모델 예측 기반 매수/매도

**결과**:
- ✅ Buy & Hold 전략 대비 **우수한 수익**
- ✅ 실전 적용 가능성 입증
- ✅ 수익성 있는 투자 도구

**중요성**:
- 좋은 예측 ≠ 좋은 트레이딩
- 실제 수익 검증 필수

---

## BTC 시계열 특성 분석

### 계절성 분해 결과

**Observed (관측)**:
- 2021-2022년 큰 변동성
- 2017~2021 상승 추세
- 2022~2023 하락 후 회복

**Trend (추세)**:
- 장기 상승 추세
- 2023년 중반 이후 회복
- 단기 변동 제거

**Seasonal (계절성)**:
- **365일 주기** 패턴 확인
- 투자자 심리, 시장 심리
- 거시경제 조건, 팬데믹
- 규제 뉴스, 기술 업데이트

**Residual (잔차)**:
- 2017-2018, 2021-2022 높은 변동성
- 예측 불가능한 시장 충격
- 랜덤 노이즈

### ACF (자기상관함수) 분석

**결과**:
- 50+ lag까지 높은 양의 상관관계
- 점진적 감소 (빠르게 0으로 안됨)
- **강한 시간 의존성 확인**
- **비정상성 확인**

**의미**:
- 과거 가격이 미래에 큰 영향
- 금융 시계열의 전형적 특성
- 장기 메모리 효과
- 정교한 모델 필요 (Helformer)

### 암호화폐 간 상관관계

**기간**: 2023.01 ~ 2024.06

**BTC와의 상관관계**:
- 대부분 코인과 **0.7 이상**
- BTC가 시장 방향 주도
- 높은 상관관계 = Transfer Learning 가능

**함의**:
- BTC 모델이 다른 코인에도 유효
- 시장 전체 움직임 동조화

---

## 핵심 혁신

### 1. 자동 계절성 처리

**기존 방법**:
- 수동 Feature Engineering
- SSA, EMD, VMD 등 분해 필요
- 시간 의존 변수 수동 생성

**Helformer**:
- ✅ 자동 학습 및 추출
- ✅ 수동 개입 불필요
- ✅ 단순화된 패턴 인식

### 2. Encoder-Only 구조

**전통 Transformer**: Encoder + Decoder

**Helformer**: Encoder만

**장점**:
- ✅ 메모리 복잡도 감소
- ✅ 계산 자원 절약
- ✅ 예측 정확도 유지

**근거**:
- 선행 연구에서 시계열 예측에는 Encoder만으로 충분
- Decoder는 NLP 작업에 주로 필요

### 3. LSTM 통합

**FFN 대체**: Feed Forward Network → LSTM

**이유**:
- 시간 의존성 포착
- 시계열 예측에 특화
- Transformer + RNN 장점 결합

### 4. Bayesian 최적화

**수동 튜닝 문제**:
- 시간 소모
- 차선 결과
- 일관성 부족

**Optuna 사용**:
- ✅ 자동 최적화
- ✅ Pruner로 효율 증대
- ✅ 강건한 예측
- ✅ 재현 가능

---

## 장점 및 기여

### 1. 이론적 기여

- ✅ 시계열 분해 + Transformer 결합
- ✅ 계절성/비정상성 자동 처리
- ✅ Encoder-only로 복잡도 감소
- ✅ LSTM 통합으로 시간 의존성 포착

### 2. 실용적 기여

- ✅ 최고 수준의 예측 정확도
- ✅ 15개 코인에서 일반화 검증
- ✅ 트레이딩 전략 수익성 입증
- ✅ 구현 상대적으로 간단

### 3. 방법론적 기여

- ✅ Bayesian 최적화 (Optuna)
- ✅ Mish 활성화 함수
- ✅ MinMax 정규화
- ✅ 체계적 실험 설계

### 4. 검증의 엄격성

- ✅ 다양한 벤치마크 모델
- ✅ Transfer Learning 검증
- ✅ 기존 연구와 비교
- ✅ 실전 트레이딩 전략

---

## 한계 및 향후 연구

### 1. 데이터 다양성 부족

**현재**: 종가(Close Price)만 사용

**개선**:
- 온체인 데이터 통합 (논문1 참조)
- 거시경제 지표
- 감정 지표
- 거래량, 변동성 등

**예상 효과**:
- 더 강건한 예측
- 외부 요인 반영

### 2. 해석 가능성

**문제**: 블랙박스 모델

**개선**:
- SHAP/LIME으로 해석
- Attention 가중치 시각화
- Feature Importance 분석

### 3. 장기 예측

**현재**: 다음날 (1일) 예측만

**확장**:
- 7일, 30일, 90일 예측
- 논문1처럼 다양한 기간

### 4. 실시간 배포

**과제**:
- 실시간 데이터 스트림
- 모델 업데이트
- 레이턴시 최소화
- 프로덕션 환경

### 5. 리스크 관리

**현재**: 단순 트레이딩 전략

**개선**:
- 포지션 사이징
- 손절/익절 규칙
- 최대 드로우다운 제한
- 켈리 기준 등

---

## 비교: 논문1 vs 논문2

| 항목 | 논문1 (Cyprus) | 논문2 (Helformer) |
|------|----------------|-------------------|
| **초점** | 데이터 다양성 | 모델 아키텍처 |
| **변수** | 429개 → 100개 | 1개 (종가) |
| **모델** | RF, XGBoost | Helformer |
| **예측 기간** | 1, 7, 30, 90, 180일 | 1일 |
| **강점** | 외부 요인, 해석성 | 자동화, 일반화 |
| **약점** | 복잡도, 데이터 수집 | 블랙박스, 단기만 |

### 상호 보완적

**논문1의 데이터 + 논문2의 모델 = 최강 조합**

```
다양한 데이터 소스
    ↓
Feature Selection (FRA)
    ↓
Helformer 모델
    ↓
최고 성능 예측
```

---

## 실용적 시사점

### 투자자를 위한 교훈

1. **딥러닝 모델 활용**
   - Helformer 같은 최신 모델
   - 전통 기술적 분석 보완

2. **Transfer Learning 활용**
   - BTC 모델로 다른 코인 예측
   - 새 코인 즉시 대응

3. **트레이딩 전략 검증 필수**
   - 좋은 예측 ≠ 좋은 수익
   - 백테스팅 필수

### 연구자를 위한 교훈

1. **시계열 분해 중요**
   - 비정상성/계절성 처리
   - 자동화 가능

2. **하이퍼파라미터 최적화**
   - Bayesian > Grid/Random
   - Optuna 같은 도구 활용

3. **일반화 검증**
   - Transfer Learning 테스트
   - 다양한 자산에서 검증

### 개발자를 위한 교훈

1. **GPU 필수**
   - Google Colab 활용
   - NVIDIA A100 권장

2. **프레임워크**
   - TensorFlow 2.17+
   - Keras
   - Optuna

3. **데이터 소스**
   - Yahoo Finance (무료)
   - 일별 종가로 시작

---

## 결론

### 핵심 메시지

> **Holt-Winters + Transformer = 암호화폐 예측의 새로운 지평**

### 주요 성과

1. ✅ **Helformer 모델** - 최고 성능
2. ✅ **계절성/비정상성 자동 처리**
3. ✅ **15개 코인 일반화** 검증
4. ✅ **트레이딩 수익성** 입증
5. ✅ **모든 벤치마크 능가**

### 실용적 가치

- 투자자: 강력한 예측 도구
- 연구자: 새로운 방법론
- 개발자: 구현 가능한 모델

### 향후 방향

- 외부 데이터 통합 (논문1)
- 장기 예측 확장
- 실시간 배포
- 리스크 관리 통합

---

## 기술 스택

### 소프트웨어
- Python 3.10.12
- TensorFlow 2.17.0
- Keras (내장)
- Optuna (Bayesian Optimization)
- Matplotlib, Seaborn (시각화)
- NumPy, Pandas (데이터 처리)

### 하드웨어
- Google Colab Premium
- NVIDIA A100 GPU
- 고성능 컴퓨팅

### 데이터
- Yahoo Finance API
- 무료, 고품질
- 널리 사용됨

---

## 재현 가능성

**논문의 강점**:
- ✅ 명확한 방법론
- ✅ 상세한 하이퍼파라미터
- ✅ 공개 데이터 (Yahoo Finance)
- ✅ 표준 프레임워크 (TensorFlow)

**재현 단계**:
1. Yahoo Finance에서 데이터 다운로드
2. Holt-Winters 분해 구현
3. Transformer Encoder 구현
4. LSTM 통합
5. Optuna로 최적화
6. 학습 및 평가

---

**요약 작성일**: 2025년
**논문 발표**: 2025년 (Journal of Big Data)
**데이터 컷오프**: 2024년 6월
**BTC 최고가 기록**: 2024년 12월 5일 ($103,900.47)
